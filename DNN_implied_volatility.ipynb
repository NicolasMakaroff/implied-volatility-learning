{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z4qMPLfTKf23"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import from_numpy, tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "JK7OeKK4KwyJ",
    "outputId": "d2a24312-ea00-4a3d-ab9f-061e34ec15d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'PES-metamodeling'...\n",
      "remote: Enumerating objects: 19, done.\u001b[K\n",
      "remote: Counting objects: 100% (19/19), done.\u001b[K\n",
      "remote: Compressing objects: 100% (14/14), done.\u001b[K\n",
      "remote: Total 19 (delta 3), reused 13 (delta 0), pack-reused 0\u001b[K\n",
      "Unpacking objects: 100% (19/19), done.\n",
      "/Users/nicolasmakaroff/Desktop/S4/PRR/implied-volatility-learning/PES-metamodeling/src\n",
      "/Users/nicolasmakaroff/Desktop/S4/PRR/implied-volatility-learning\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/NicolasMakaroff/PES-metamodeling.git\n",
    "%cd PES-metamodeling/src/\n",
    "sys.path.append(os.getcwd())\n",
    "%cd ../.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 75
    },
    "colab_type": "code",
    "id": "iuvz7NPXLQdS",
    "outputId": "eb1364d5-e9fa-4915-eb9f-631e256cdc60"
   },
   "outputs": [],
   "source": [
    "from data_gestion import open_data, norm, create_train_test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XrXpk-3ULXAN"
   },
   "outputs": [],
   "source": [
    "database = open_data('data/implied-volatility-reduce.csv')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gFwyDN4pLnDJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              So       tau         r     price\n",
      "373315  1.075941  0.731160  0.078551  0.145111\n",
      "459286  1.266984  0.879115  0.020342  0.409147\n",
      "262398  1.294953  0.710363  0.008480  0.427321\n",
      "789396  1.067558  0.897104  0.035051  0.317528\n",
      "383229  0.747197  0.827045  0.034583  0.009318\n"
     ]
    }
   ],
   "source": [
    "train_features, train_labels, test_features, test_labels = create_train_test_set(database,0.9,1.,'sigma')  \n",
    "print(train_features.head())\n",
    "normed_train_features, normed_test_features = norm(train_features), norm(test_features)\n",
    "train_dataset = pd.concat([normed_train_features, train_labels], axis=1)\n",
    "test_dataset = pd.concat([normed_test_features, test_labels], axis=1)\n",
    "         \n",
    "train_data = train_dataset.to_numpy(dtype=np.float32) \n",
    "test_data = test_dataset.to_numpy(dtype=np.float32)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 56
    },
    "colab_type": "code",
    "id": "QENwuVTRqjRl",
    "outputId": "1c6ae879-e5b9-43a7-8282-75f0273c812e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"from sklearn.preprocessing import PolynomialFeatures\\nfrom sklearn.metrics import mean_squared_error\\nfrom sklearn import linear_model\\nfor i in range(7,20):\\n  polyn = PolynomialFeatures(degree=i)\\n  x_ = polyn.fit_transform(normed_train_features) #x_ contient les degrés et produits croisés\\n  #Une fois qu'on a préparé les degrés on peut faire la régression\\n  clf = linear_model.LinearRegression()\\n  clf.fit(x_,train_labels)\\n  y_ = polyn.fit_transform(normed_test_features)\\n  y1 = clf.predict(y_)\\n  print('RMSE : {} and degree : {}' .format(mean_squared_error(y1,test_labels),i))\""
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "  \"\"\"from sklearn.preprocessing import PolynomialFeatures\n",
    "  from sklearn.metrics import mean_squared_error\n",
    "  from sklearn import linear_model\n",
    "  for i in range(7,20):\n",
    "    polyn = PolynomialFeatures(degree=i)\n",
    "    x_ = polyn.fit_transform(normed_train_features) #x_ contient les degrés et produits croisés\n",
    "    #Une fois qu'on a préparé les degrés on peut faire la régression\n",
    "    clf = linear_model.LinearRegression()\n",
    "    clf.fit(x_,train_labels)\n",
    "    y_ = polyn.fit_transform(normed_test_features)\n",
    "    y1 = clf.predict(y_)\n",
    "    print('RMSE : {} and degree : {}' .format(mean_squared_error(y1,test_labels),i))\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eRpDI1xpOJeM"
   },
   "outputs": [],
   "source": [
    "class VolatilityDataset(Dataset):\n",
    "    \"\"\" PES dataset.\"\"\"\n",
    "\n",
    "    # Initialize your data, download, etc.\n",
    "    def __init__(self,data):\n",
    "        xy = data\n",
    "        self.len = xy.shape[0]\n",
    "        self.x_data = from_numpy(xy[:, 0:4])\n",
    "        self.y_data = from_numpy(xy[:, [-1]])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6ybB1JKsPwlC"
   },
   "outputs": [],
   "source": [
    "train_dataset = VolatilityDataset(train_data)\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=1024,\n",
    "                          shuffle=True,\n",
    "                          num_workers=2)\n",
    "\n",
    "test_dataset = VolatilityDataset(test_data)\n",
    "test_loader = DataLoader(dataset=test_dataset,\n",
    "                          batch_size=1024,\n",
    "                          shuffle=True,\n",
    "                          num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xE7eVq3PQHvq"
   },
   "outputs": [],
   "source": [
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Regressor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(4, 400)\n",
    "        #self.bn1 = nn.BatchNorm1d(600)\n",
    "        self.fc2 = nn.Linear(400, 400)\n",
    "        #self.bn2 = nn.BatchNorm1d(500)\n",
    "        self.fc3 = nn.Linear(400, 400)\n",
    "        #self.bn3 = nn.BatchNorm1d(400)\n",
    "        self.fc4 = nn.Linear(400, 400)\n",
    "        #self.bn4 = nn.BatchNorm1d(300)\n",
    "        self.fc5 = nn.Linear(400,400)\n",
    "        #self.bn5 = nn.BatchNorm1d(200)\n",
    "        self.fc6 = nn.Linear(400,1)\n",
    "        \n",
    "        self.drop = nn.Dropout(p=0.25)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = F.relu(self.fc5(x))\n",
    "\n",
    "        x = self.fc6(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 189
    },
    "colab_type": "code",
    "id": "mY95Ss_N9abC",
    "outputId": "eec01489-31ed-4c4c-c66d-1a72a82c4071"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'adatune'...\n",
      "remote: Enumerating objects: 22, done.\u001b[K\n",
      "remote: Counting objects: 100% (22/22), done.\u001b[K\n",
      "remote: Compressing objects: 100% (19/19), done.\u001b[K\n",
      "remote: Total 82 (delta 8), reused 9 (delta 3), pack-reused 60\u001b[K\n",
      "Unpacking objects: 100% (82/82), done.\n",
      "/content/adatune\n",
      "/bin/bash: -c: line 0: unexpected EOF while looking for matching `\"'\n",
      "/bin/bash: -c: line 1: syntax error: unexpected end of file\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/awslabs/adatune.git\n",
    "%cd adatune\n",
    "!python setup.py install\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 228
    },
    "colab_type": "code",
    "id": "pfLxIrbjkd4a",
    "outputId": "3aae6380-f46a-43f5-90c7-ffe556691ca8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/gbaydin/hypergradient-descent.git\n",
      "  Cloning https://github.com/gbaydin/hypergradient-descent.git to /tmp/pip-req-build-thkgy4ec\n",
      "  Running command git clone -q https://github.com/gbaydin/hypergradient-descent.git /tmp/pip-req-build-thkgy4ec\n",
      "Building wheels for collected packages: hypergrad\n",
      "  Building wheel for hypergrad (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for hypergrad: filename=hypergrad-0.1-cp36-none-any.whl size=8188 sha256=949a8b9451bee3ce4bdd50f388c702ee4d90d1fa034e75d31d549578a6249f4e\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-1_u8t65n/wheels/fd/a2/2d/63c4e652ba00bf75e1e0e9ce9628cb47c151bd10322a6cf978\n",
      "Successfully built hypergrad\n",
      "Installing collected packages: hypergrad\n",
      "Successfully installed hypergrad-0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/gbaydin/hypergradient-descent.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ik_5ELiO9iLC"
   },
   "outputs": [],
   "source": [
    "#from hypergrad import SGDHD, AdamHD\n",
    "from adatune.mu_adam import MuAdam\n",
    "from adatune.network import *\n",
    "from adatune.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_Lc1iHBURB1U"
   },
   "outputs": [],
   "source": [
    "model = Regressor()\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(),lr=0.001)\n",
    "#hyper_optim = MuAdam(optimizer, 1e-6, 1000.0, 'store_true', 0.99999, 1e-6, 'cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "yB4mNa9ERctI",
    "outputId": "47d58217-f72f-463d-eb73-ff67302a0ca4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/1500..  Training Loss: -5.297..  Test Loss: -6.495..  Learning rate: 0.001..\n",
      "Validation loss decreased (inf --> 0.0015109744854271412).  Saving model ...\n",
      "Epoch: 2/1500..  Training Loss: -6.444..  Test Loss: -6.629..  Learning rate: 0.001..\n",
      "Validation loss decreased (0.0015109744854271412 --> 0.001321165356785059).  Saving model ...\n",
      "Epoch: 3/1500..  Training Loss: -6.536..  Test Loss: -6.565..  Learning rate: 0.001..\n",
      "Epoch: 4/1500..  Training Loss: -6.581..  Test Loss: -6.821..  Learning rate: 0.001..\n",
      "Validation loss decreased (0.001321165356785059 --> 0.0010905048111453652).  Saving model ...\n",
      "Epoch: 5/1500..  Training Loss: -6.628..  Test Loss: -6.569..  Learning rate: 0.001..\n",
      "Epoch: 6/1500..  Training Loss: -6.643..  Test Loss: -6.877..  Learning rate: 0.001..\n",
      "Validation loss decreased (0.0010905048111453652 --> 0.0010315345134586096).  Saving model ...\n",
      "Epoch: 7/1500..  Training Loss: -6.642..  Test Loss: -6.824..  Learning rate: 0.001..\n",
      "Epoch: 8/1500..  Training Loss: -6.683..  Test Loss: -6.335..  Learning rate: 0.001..\n",
      "Epoch: 9/1500..  Training Loss: -6.701..  Test Loss: -6.733..  Learning rate: 0.001..\n",
      "Epoch: 10/1500..  Training Loss: -6.735..  Test Loss: -6.972..  Learning rate: 0.001..\n",
      "Validation loss decreased (0.0010315345134586096 --> 0.0009381884010508657).  Saving model ...\n",
      "Epoch: 11/1500..  Training Loss: -6.726..  Test Loss: -6.726..  Learning rate: 0.001..\n",
      "Epoch: 12/1500..  Training Loss: -6.734..  Test Loss: -6.760..  Learning rate: 0.001..\n",
      "Epoch: 13/1500..  Training Loss: -6.778..  Test Loss: -6.584..  Learning rate: 0.001..\n",
      "Epoch: 14/1500..  Training Loss: -6.781..  Test Loss: -6.650..  Learning rate: 0.001..\n",
      "Epoch: 15/1500..  Training Loss: -6.790..  Test Loss: -6.512..  Learning rate: 0.001..\n",
      "Epoch: 16/1500..  Training Loss: -6.817..  Test Loss: -6.577..  Learning rate: 0.001..\n",
      "Epoch: 17/1500..  Training Loss: -6.836..  Test Loss: -6.952..  Learning rate: 0.001..\n",
      "Epoch: 18/1500..  Training Loss: -6.854..  Test Loss: -6.851..  Learning rate: 0.001..\n",
      "Epoch: 19/1500..  Training Loss: -6.776..  Test Loss: -6.925..  Learning rate: 0.001..\n",
      "Epoch: 20/1500..  Training Loss: -6.871..  Test Loss: -6.611..  Learning rate: 0.001..\n",
      "Epoch: 21/1500..  Training Loss: -6.835..  Test Loss: -6.809..  Learning rate: 0.001..\n",
      "Epoch: 22/1500..  Training Loss: -6.847..  Test Loss: -6.871..  Learning rate: 0.001..\n",
      "Epoch: 23/1500..  Training Loss: -6.888..  Test Loss: -6.698..  Learning rate: 0.001..\n",
      "Epoch: 24/1500..  Training Loss: -6.931..  Test Loss: -6.917..  Learning rate: 0.001..\n",
      "Epoch: 25/1500..  Training Loss: -6.866..  Test Loss: -6.997..  Learning rate: 0.001..\n",
      "Validation loss decreased (0.0009381884010508657 --> 0.0009144034120254219).  Saving model ...\n",
      "Epoch: 26/1500..  Training Loss: -6.860..  Test Loss: -6.949..  Learning rate: 0.001..\n",
      "Epoch: 27/1500..  Training Loss: -6.861..  Test Loss: -7.030..  Learning rate: 0.001..\n",
      "Validation loss decreased (0.0009144034120254219 --> 0.0008845022530294955).  Saving model ...\n",
      "Epoch: 28/1500..  Training Loss: -6.913..  Test Loss: -6.847..  Learning rate: 0.001..\n",
      "Epoch: 29/1500..  Training Loss: -6.875..  Test Loss: -6.902..  Learning rate: 0.001..\n",
      "Epoch: 30/1500..  Training Loss: -6.923..  Test Loss: -6.975..  Learning rate: 0.001..\n",
      "Epoch: 31/1500..  Training Loss: -6.947..  Test Loss: -6.854..  Learning rate: 0.001..\n",
      "Epoch: 32/1500..  Training Loss: -6.924..  Test Loss: -6.478..  Learning rate: 0.001..\n",
      "Epoch: 33/1500..  Training Loss: -6.892..  Test Loss: -6.466..  Learning rate: 0.001..\n",
      "Epoch: 34/1500..  Training Loss: -6.942..  Test Loss: -6.927..  Learning rate: 0.001..\n",
      "Epoch: 35/1500..  Training Loss: -6.923..  Test Loss: -6.506..  Learning rate: 0.001..\n",
      "Epoch: 36/1500..  Training Loss: -6.960..  Test Loss: -6.784..  Learning rate: 0.001..\n",
      "Epoch: 37/1500..  Training Loss: -6.969..  Test Loss: -6.928..  Learning rate: 0.001..\n",
      "Epoch: 38/1500..  Training Loss: -6.953..  Test Loss: -7.037..  Learning rate: 0.001..\n",
      "Validation loss decreased (0.0008845022530294955 --> 0.0008787977858446538).  Saving model ...\n",
      "Epoch: 39/1500..  Training Loss: -6.957..  Test Loss: -6.848..  Learning rate: 0.001..\n",
      "Epoch: 40/1500..  Training Loss: -6.927..  Test Loss: -7.146..  Learning rate: 0.001..\n",
      "Validation loss decreased (0.0008787977858446538 --> 0.0007877380703575909).  Saving model ...\n",
      "Epoch: 41/1500..  Training Loss: -6.979..  Test Loss: -6.942..  Learning rate: 0.001..\n",
      "Epoch: 42/1500..  Training Loss: -6.903..  Test Loss: -7.009..  Learning rate: 0.001..\n",
      "Epoch: 43/1500..  Training Loss: -6.925..  Test Loss: -7.078..  Learning rate: 0.001..\n",
      "Epoch: 44/1500..  Training Loss: -6.948..  Test Loss: -7.048..  Learning rate: 0.001..\n",
      "Epoch: 45/1500..  Training Loss: -6.918..  Test Loss: -6.784..  Learning rate: 0.001..\n",
      "Epoch: 46/1500..  Training Loss: -6.948..  Test Loss: -6.182..  Learning rate: 0.001..\n",
      "Epoch: 47/1500..  Training Loss: -6.970..  Test Loss: -7.055..  Learning rate: 0.001..\n",
      "Epoch: 48/1500..  Training Loss: -6.973..  Test Loss: -7.074..  Learning rate: 0.001..\n",
      "Epoch: 49/1500..  Training Loss: -6.927..  Test Loss: -6.434..  Learning rate: 0.001..\n",
      "Epoch: 50/1500..  Training Loss: -6.882..  Test Loss: -7.096..  Learning rate: 0.001..\n",
      "Epoch: 51/1500..  Training Loss: -6.954..  Test Loss: -6.869..  Learning rate: 0.001..\n",
      "Epoch: 52/1500..  Training Loss: -6.929..  Test Loss: -6.838..  Learning rate: 0.001..\n",
      "Epoch: 53/1500..  Training Loss: -6.898..  Test Loss: -6.999..  Learning rate: 0.001..\n",
      "Epoch: 54/1500..  Training Loss: -6.971..  Test Loss: -6.603..  Learning rate: 0.001..\n",
      "Epoch: 55/1500..  Training Loss: -6.939..  Test Loss: -7.093..  Learning rate: 0.001..\n",
      "Epoch: 56/1500..  Training Loss: -6.934..  Test Loss: -6.865..  Learning rate: 0.001..\n",
      "Epoch: 57/1500..  Training Loss: -6.910..  Test Loss: -7.097..  Learning rate: 0.001..\n",
      "Epoch: 58/1500..  Training Loss: -6.961..  Test Loss: -7.019..  Learning rate: 0.001..\n",
      "Epoch: 59/1500..  Training Loss: -6.987..  Test Loss: -7.142..  Learning rate: 0.001..\n",
      "Epoch: 60/1500..  Training Loss: -6.968..  Test Loss: -7.123..  Learning rate: 0.001..\n",
      "Epoch: 61/1500..  Training Loss: -6.956..  Test Loss: -7.139..  Learning rate: 0.001..\n",
      "Epoch: 62/1500..  Training Loss: -6.944..  Test Loss: -7.154..  Learning rate: 0.001..\n",
      "Validation loss decreased (0.0007877380703575909 --> 0.0007815897697582841).  Saving model ...\n",
      "Epoch: 63/1500..  Training Loss: -7.007..  Test Loss: -7.181..  Learning rate: 0.001..\n",
      "Validation loss decreased (0.0007815897697582841 --> 0.0007609726162627339).  Saving model ...\n",
      "Epoch: 64/1500..  Training Loss: -6.967..  Test Loss: -7.048..  Learning rate: 0.001..\n",
      "Epoch: 65/1500..  Training Loss: -7.018..  Test Loss: -6.969..  Learning rate: 0.001..\n",
      "Epoch: 66/1500..  Training Loss: -6.870..  Test Loss: -6.858..  Learning rate: 0.001..\n",
      "Epoch: 67/1500..  Training Loss: -6.970..  Test Loss: -6.608..  Learning rate: 0.001..\n",
      "Epoch: 68/1500..  Training Loss: -6.965..  Test Loss: -7.038..  Learning rate: 0.001..\n",
      "Epoch: 69/1500..  Training Loss: -6.928..  Test Loss: -6.922..  Learning rate: 0.001..\n",
      "Epoch: 70/1500..  Training Loss: -6.973..  Test Loss: -7.081..  Learning rate: 0.001..\n",
      "Epoch: 71/1500..  Training Loss: -6.980..  Test Loss: -7.066..  Learning rate: 0.001..\n",
      "Epoch: 72/1500..  Training Loss: -7.042..  Test Loss: -6.988..  Learning rate: 0.001..\n",
      "Epoch: 73/1500..  Training Loss: -6.966..  Test Loss: -6.819..  Learning rate: 0.001..\n",
      "Epoch: 74/1500..  Training Loss: -7.021..  Test Loss: -7.114..  Learning rate: 0.001..\n",
      "Epoch: 75/1500..  Training Loss: -7.057..  Test Loss: -7.049..  Learning rate: 0.001..\n",
      "Epoch: 76/1500..  Training Loss: -7.037..  Test Loss: -6.499..  Learning rate: 0.001..\n",
      "Epoch: 77/1500..  Training Loss: -6.979..  Test Loss: -6.990..  Learning rate: 0.001..\n",
      "Epoch: 78/1500..  Training Loss: -7.027..  Test Loss: -6.868..  Learning rate: 0.001..\n",
      "Epoch: 79/1500..  Training Loss: -6.976..  Test Loss: -7.119..  Learning rate: 0.001..\n",
      "Epoch: 80/1500..  Training Loss: -6.988..  Test Loss: -7.235..  Learning rate: 0.001..\n",
      "Validation loss decreased (0.0007609726162627339 --> 0.0007211656775325537).  Saving model ...\n",
      "Epoch: 81/1500..  Training Loss: -7.004..  Test Loss: -7.003..  Learning rate: 0.001..\n",
      "Epoch: 82/1500..  Training Loss: -7.057..  Test Loss: -6.894..  Learning rate: 0.001..\n",
      "Epoch: 83/1500..  Training Loss: -7.049..  Test Loss: -7.165..  Learning rate: 0.001..\n",
      "Epoch: 84/1500..  Training Loss: -6.970..  Test Loss: -7.200..  Learning rate: 0.001..\n",
      "Epoch: 85/1500..  Training Loss: -7.058..  Test Loss: -6.940..  Learning rate: 0.001..\n",
      "Epoch: 86/1500..  Training Loss: -7.057..  Test Loss: -7.209..  Learning rate: 0.001..\n",
      "Epoch: 87/1500..  Training Loss: -7.001..  Test Loss: -7.081..  Learning rate: 0.001..\n",
      "Epoch: 88/1500..  Training Loss: -7.037..  Test Loss: -7.068..  Learning rate: 0.001..\n",
      "Epoch: 89/1500..  Training Loss: -7.052..  Test Loss: -7.103..  Learning rate: 0.001..\n",
      "Epoch: 90/1500..  Training Loss: -7.057..  Test Loss: -7.129..  Learning rate: 0.001..\n",
      "Epoch: 91/1500..  Training Loss: -7.005..  Test Loss: -6.888..  Learning rate: 0.001..\n",
      "Epoch: 92/1500..  Training Loss: -7.040..  Test Loss: -7.206..  Learning rate: 0.001..\n",
      "Epoch: 93/1500..  Training Loss: -7.038..  Test Loss: -7.050..  Learning rate: 0.001..\n",
      "Epoch: 94/1500..  Training Loss: -7.051..  Test Loss: -7.185..  Learning rate: 0.001..\n",
      "Epoch: 95/1500..  Training Loss: -7.011..  Test Loss: -6.930..  Learning rate: 0.001..\n",
      "Epoch: 96/1500..  Training Loss: -7.034..  Test Loss: -7.156..  Learning rate: 0.001..\n",
      "Epoch: 97/1500..  Training Loss: -7.038..  Test Loss: -6.773..  Learning rate: 0.001..\n",
      "Epoch: 98/1500..  Training Loss: -7.043..  Test Loss: -7.069..  Learning rate: 0.001..\n",
      "Epoch: 99/1500..  Training Loss: -7.000..  Test Loss: -6.927..  Learning rate: 0.001..\n",
      "Epoch: 100/1500..  Training Loss: -7.039..  Test Loss: -6.957..  Learning rate: 0.001..\n",
      "Epoch: 101/1500..  Training Loss: -7.031..  Test Loss: -7.219..  Learning rate: 0.001..\n",
      "Epoch: 102/1500..  Training Loss: -7.068..  Test Loss: -7.227..  Learning rate: 0.001..\n",
      "Epoch: 103/1500..  Training Loss: -7.041..  Test Loss: -6.772..  Learning rate: 0.001..\n",
      "Epoch: 104/1500..  Training Loss: -7.060..  Test Loss: -6.663..  Learning rate: 0.001..\n",
      "Epoch: 105/1500..  Training Loss: -7.045..  Test Loss: -6.993..  Learning rate: 0.001..\n",
      "Epoch: 106/1500..  Training Loss: -7.048..  Test Loss: -7.162..  Learning rate: 0.001..\n",
      "Epoch: 107/1500..  Training Loss: -7.043..  Test Loss: -7.104..  Learning rate: 0.001..\n",
      "Epoch: 108/1500..  Training Loss: -7.090..  Test Loss: -7.246..  Learning rate: 0.001..\n",
      "Validation loss decreased (0.0007211656775325537 --> 0.000713178887963295).  Saving model ...\n",
      "Epoch: 109/1500..  Training Loss: -7.107..  Test Loss: -6.751..  Learning rate: 0.001..\n",
      "Epoch: 110/1500..  Training Loss: -7.031..  Test Loss: -7.057..  Learning rate: 0.001..\n",
      "Epoch: 111/1500..  Training Loss: -7.053..  Test Loss: -6.625..  Learning rate: 0.001..\n",
      "Epoch: 112/1500..  Training Loss: -7.108..  Test Loss: -7.162..  Learning rate: 0.001..\n",
      "Epoch: 113/1500..  Training Loss: -6.993..  Test Loss: -6.542..  Learning rate: 0.001..\n",
      "Epoch: 114/1500..  Training Loss: -6.976..  Test Loss: -7.119..  Learning rate: 0.001..\n",
      "Epoch: 115/1500..  Training Loss: -7.098..  Test Loss: -7.202..  Learning rate: 0.001..\n",
      "Epoch: 116/1500..  Training Loss: -7.109..  Test Loss: -6.916..  Learning rate: 0.001..\n",
      "Epoch: 117/1500..  Training Loss: -7.040..  Test Loss: -7.203..  Learning rate: 0.001..\n",
      "Epoch: 118/1500..  Training Loss: -7.038..  Test Loss: -7.153..  Learning rate: 0.001..\n",
      "Epoch: 119/1500..  Training Loss: -7.092..  Test Loss: -7.274..  Learning rate: 0.001..\n",
      "Validation loss decreased (0.000713178887963295 --> 0.0006932904361747205).  Saving model ...\n",
      "Epoch: 120/1500..  Training Loss: -7.130..  Test Loss: -7.095..  Learning rate: 0.001..\n",
      "Epoch: 121/1500..  Training Loss: -7.031..  Test Loss: -7.215..  Learning rate: 0.001..\n",
      "Epoch: 122/1500..  Training Loss: -7.022..  Test Loss: -7.269..  Learning rate: 0.001..\n",
      "Epoch: 123/1500..  Training Loss: -6.985..  Test Loss: -6.899..  Learning rate: 0.001..\n",
      "Epoch: 124/1500..  Training Loss: -7.069..  Test Loss: -7.202..  Learning rate: 0.001..\n",
      "Epoch: 125/1500..  Training Loss: -7.089..  Test Loss: -6.964..  Learning rate: 0.001..\n",
      "Epoch: 126/1500..  Training Loss: -7.108..  Test Loss: -7.256..  Learning rate: 0.001..\n",
      "Epoch: 127/1500..  Training Loss: -7.025..  Test Loss: -7.197..  Learning rate: 0.001..\n",
      "Epoch: 128/1500..  Training Loss: -7.043..  Test Loss: -6.884..  Learning rate: 0.001..\n",
      "Epoch: 129/1500..  Training Loss: -7.066..  Test Loss: -7.079..  Learning rate: 0.001..\n",
      "Epoch: 130/1500..  Training Loss: -7.094..  Test Loss: -7.055..  Learning rate: 0.001..\n",
      "Epoch: 131/1500..  Training Loss: -7.104..  Test Loss: -7.114..  Learning rate: 0.001..\n",
      "Epoch: 132/1500..  Training Loss: -7.097..  Test Loss: -7.145..  Learning rate: 0.001..\n",
      "Epoch: 133/1500..  Training Loss: -7.076..  Test Loss: -6.903..  Learning rate: 0.001..\n",
      "Epoch: 134/1500..  Training Loss: -7.105..  Test Loss: -7.013..  Learning rate: 0.001..\n",
      "Epoch: 135/1500..  Training Loss: -6.996..  Test Loss: -7.070..  Learning rate: 0.001..\n",
      "Epoch: 136/1500..  Training Loss: -7.046..  Test Loss: -7.217..  Learning rate: 0.001..\n",
      "Epoch: 137/1500..  Training Loss: -7.124..  Test Loss: -7.228..  Learning rate: 0.001..\n",
      "Epoch: 138/1500..  Training Loss: -7.086..  Test Loss: -7.267..  Learning rate: 0.001..\n",
      "Epoch: 139/1500..  Training Loss: -7.094..  Test Loss: -7.037..  Learning rate: 0.001..\n",
      "Epoch: 140/1500..  Training Loss: -7.067..  Test Loss: -7.194..  Learning rate: 0.001..\n",
      "Epoch: 141/1500..  Training Loss: -7.062..  Test Loss: -6.696..  Learning rate: 0.001..\n",
      "Epoch: 142/1500..  Training Loss: -7.081..  Test Loss: -6.892..  Learning rate: 0.001..\n",
      "Epoch: 143/1500..  Training Loss: -7.059..  Test Loss: -7.190..  Learning rate: 0.001..\n",
      "Epoch: 144/1500..  Training Loss: -7.086..  Test Loss: -7.135..  Learning rate: 0.001..\n",
      "Epoch: 145/1500..  Training Loss: -7.045..  Test Loss: -6.884..  Learning rate: 0.001..\n",
      "Epoch: 146/1500..  Training Loss: -7.098..  Test Loss: -7.114..  Learning rate: 0.001..\n",
      "Epoch: 147/1500..  Training Loss: -7.111..  Test Loss: -7.079..  Learning rate: 0.001..\n",
      "Epoch: 148/1500..  Training Loss: -7.067..  Test Loss: -7.189..  Learning rate: 0.001..\n",
      "Epoch: 149/1500..  Training Loss: -7.077..  Test Loss: -7.116..  Learning rate: 0.001..\n",
      "Epoch: 150/1500..  Training Loss: -7.131..  Test Loss: -6.583..  Learning rate: 0.001..\n",
      "Epoch: 151/1500..  Training Loss: -7.031..  Test Loss: -7.015..  Learning rate: 0.001..\n",
      "Epoch: 152/1500..  Training Loss: -7.033..  Test Loss: -7.204..  Learning rate: 0.001..\n",
      "Epoch: 153/1500..  Training Loss: -7.087..  Test Loss: -7.108..  Learning rate: 0.001..\n",
      "Epoch: 154/1500..  Training Loss: -7.086..  Test Loss: -7.014..  Learning rate: 0.001..\n",
      "Epoch: 155/1500..  Training Loss: -7.111..  Test Loss: -7.283..  Learning rate: 0.001..\n",
      "Validation loss decreased (0.0006932904361747205 --> 0.0006872125086374581).  Saving model ...\n",
      "Epoch: 156/1500..  Training Loss: -7.039..  Test Loss: -7.217..  Learning rate: 0.001..\n",
      "Epoch: 157/1500..  Training Loss: -7.054..  Test Loss: -6.893..  Learning rate: 0.001..\n",
      "Epoch: 158/1500..  Training Loss: -7.050..  Test Loss: -7.161..  Learning rate: 0.001..\n",
      "Epoch: 159/1500..  Training Loss: -7.014..  Test Loss: -7.123..  Learning rate: 0.001..\n",
      "Epoch: 160/1500..  Training Loss: -7.083..  Test Loss: -7.247..  Learning rate: 0.001..\n",
      "Epoch: 161/1500..  Training Loss: -7.132..  Test Loss: -6.671..  Learning rate: 0.001..\n",
      "Epoch: 162/1500..  Training Loss: -7.124..  Test Loss: -7.107..  Learning rate: 0.001..\n",
      "Epoch: 163/1500..  Training Loss: -7.109..  Test Loss: -7.216..  Learning rate: 0.001..\n",
      "Epoch: 164/1500..  Training Loss: -7.100..  Test Loss: -7.105..  Learning rate: 0.001..\n",
      "Epoch: 165/1500..  Training Loss: -7.102..  Test Loss: -7.054..  Learning rate: 0.001..\n",
      "Epoch: 166/1500..  Training Loss: -7.056..  Test Loss: -7.033..  Learning rate: 0.001..\n",
      "Epoch: 167/1500..  Training Loss: -7.121..  Test Loss: -7.250..  Learning rate: 0.001..\n",
      "Epoch: 168/1500..  Training Loss: -7.066..  Test Loss: -7.042..  Learning rate: 0.001..\n",
      "Epoch: 169/1500..  Training Loss: -7.105..  Test Loss: -7.240..  Learning rate: 0.001..\n",
      "Epoch: 170/1500..  Training Loss: -7.111..  Test Loss: -6.857..  Learning rate: 0.001..\n",
      "Epoch: 171/1500..  Training Loss: -7.129..  Test Loss: -7.197..  Learning rate: 0.001..\n",
      "Epoch: 172/1500..  Training Loss: -7.081..  Test Loss: -6.955..  Learning rate: 0.001..\n",
      "Epoch: 173/1500..  Training Loss: -7.102..  Test Loss: -7.225..  Learning rate: 0.001..\n",
      "Epoch: 174/1500..  Training Loss: -7.068..  Test Loss: -7.095..  Learning rate: 0.001..\n",
      "Epoch: 175/1500..  Training Loss: -7.115..  Test Loss: -7.262..  Learning rate: 0.001..\n",
      "Epoch: 176/1500..  Training Loss: -7.151..  Test Loss: -7.171..  Learning rate: 0.001..\n",
      "Epoch: 177/1500..  Training Loss: -7.116..  Test Loss: -7.264..  Learning rate: 0.001..\n",
      "Epoch: 178/1500..  Training Loss: -7.141..  Test Loss: -7.047..  Learning rate: 0.001..\n",
      "Epoch: 179/1500..  Training Loss: -7.110..  Test Loss: -6.865..  Learning rate: 0.001..\n",
      "Epoch: 180/1500..  Training Loss: -7.069..  Test Loss: -7.123..  Learning rate: 0.001..\n",
      "Epoch: 181/1500..  Training Loss: -7.116..  Test Loss: -7.170..  Learning rate: 0.001..\n",
      "Epoch: 182/1500..  Training Loss: -7.110..  Test Loss: -7.291..  Learning rate: 0.001..\n",
      "Validation loss decreased (0.0006872125086374581 --> 0.0006815230590291321).  Saving model ...\n",
      "Epoch: 183/1500..  Training Loss: -7.118..  Test Loss: -7.289..  Learning rate: 0.001..\n",
      "Epoch: 184/1500..  Training Loss: -7.134..  Test Loss: -7.128..  Learning rate: 0.001..\n",
      "Epoch: 185/1500..  Training Loss: -7.166..  Test Loss: -7.143..  Learning rate: 0.001..\n",
      "Epoch: 186/1500..  Training Loss: -7.062..  Test Loss: -7.239..  Learning rate: 0.001..\n",
      "Epoch: 187/1500..  Training Loss: -7.104..  Test Loss: -7.103..  Learning rate: 0.001..\n",
      "Epoch: 188/1500..  Training Loss: -7.127..  Test Loss: -7.170..  Learning rate: 0.001..\n",
      "Epoch: 189/1500..  Training Loss: -7.125..  Test Loss: -7.262..  Learning rate: 0.001..\n",
      "Epoch: 190/1500..  Training Loss: -7.182..  Test Loss: -7.234..  Learning rate: 0.001..\n",
      "Epoch: 191/1500..  Training Loss: -7.106..  Test Loss: -7.254..  Learning rate: 0.001..\n",
      "Epoch: 192/1500..  Training Loss: -7.066..  Test Loss: -7.236..  Learning rate: 0.001..\n",
      "Epoch: 193/1500..  Training Loss: -7.151..  Test Loss: -6.949..  Learning rate: 0.001..\n",
      "Epoch: 194/1500..  Training Loss: -7.135..  Test Loss: -7.012..  Learning rate: 0.001..\n",
      "Epoch: 195/1500..  Training Loss: -7.067..  Test Loss: -6.764..  Learning rate: 0.001..\n",
      "Epoch: 196/1500..  Training Loss: -7.118..  Test Loss: -6.801..  Learning rate: 0.001..\n",
      "Epoch: 197/1500..  Training Loss: -7.157..  Test Loss: -7.067..  Learning rate: 0.001..\n",
      "Epoch: 198/1500..  Training Loss: -7.077..  Test Loss: -7.282..  Learning rate: 0.001..\n",
      "Epoch: 199/1500..  Training Loss: -7.092..  Test Loss: -6.936..  Learning rate: 0.001..\n",
      "Epoch: 200/1500..  Training Loss: -7.103..  Test Loss: -7.066..  Learning rate: 0.001..\n",
      "Epoch: 201/1500..  Training Loss: -7.130..  Test Loss: -7.270..  Learning rate: 0.001..\n",
      "Epoch: 202/1500..  Training Loss: -7.107..  Test Loss: -6.902..  Learning rate: 0.001..\n",
      "Epoch: 203/1500..  Training Loss: -7.109..  Test Loss: -7.007..  Learning rate: 0.001..\n",
      "Epoch: 204/1500..  Training Loss: -7.118..  Test Loss: -7.166..  Learning rate: 0.001..\n",
      "Epoch: 205/1500..  Training Loss: -7.100..  Test Loss: -6.834..  Learning rate: 0.001..\n",
      "Epoch: 206/1500..  Training Loss: -7.129..  Test Loss: -7.229..  Learning rate: 0.001..\n",
      "Epoch: 207/1500..  Training Loss: -7.138..  Test Loss: -6.876..  Learning rate: 0.001..\n",
      "Epoch: 208/1500..  Training Loss: -7.111..  Test Loss: -7.302..  Learning rate: 0.001..\n",
      "Validation loss decreased (0.0006815230590291321 --> 0.0006740009412169456).  Saving model ...\n",
      "Epoch: 209/1500..  Training Loss: -7.149..  Test Loss: -7.275..  Learning rate: 0.001..\n",
      "Epoch: 210/1500..  Training Loss: -7.043..  Test Loss: -7.227..  Learning rate: 0.001..\n",
      "Epoch: 211/1500..  Training Loss: -7.172..  Test Loss: -7.209..  Learning rate: 0.001..\n",
      "Epoch: 212/1500..  Training Loss: -7.057..  Test Loss: -7.221..  Learning rate: 0.001..\n",
      "Epoch: 213/1500..  Training Loss: -7.036..  Test Loss: -7.199..  Learning rate: 0.001..\n",
      "Epoch: 214/1500..  Training Loss: -7.091..  Test Loss: -7.167..  Learning rate: 0.001..\n",
      "Epoch: 215/1500..  Training Loss: -7.157..  Test Loss: -6.634..  Learning rate: 0.001..\n",
      "Epoch: 216/1500..  Training Loss: -7.079..  Test Loss: -7.276..  Learning rate: 0.001..\n",
      "Epoch: 217/1500..  Training Loss: -7.152..  Test Loss: -6.936..  Learning rate: 0.001..\n",
      "Epoch: 218/1500..  Training Loss: -7.125..  Test Loss: -6.647..  Learning rate: 0.001..\n",
      "Epoch: 219/1500..  Training Loss: -7.168..  Test Loss: -7.210..  Learning rate: 0.001..\n",
      "Epoch: 220/1500..  Training Loss: -7.127..  Test Loss: -6.554..  Learning rate: 0.001..\n",
      "Epoch: 221/1500..  Training Loss: -7.086..  Test Loss: -7.144..  Learning rate: 0.001..\n",
      "Epoch: 222/1500..  Training Loss: -7.161..  Test Loss: -7.121..  Learning rate: 0.001..\n",
      "Epoch: 223/1500..  Training Loss: -7.109..  Test Loss: -7.040..  Learning rate: 0.001..\n",
      "Epoch: 224/1500..  Training Loss: -7.125..  Test Loss: -6.955..  Learning rate: 0.001..\n",
      "Epoch: 225/1500..  Training Loss: -7.150..  Test Loss: -6.889..  Learning rate: 0.001..\n",
      "Epoch: 226/1500..  Training Loss: -7.084..  Test Loss: -7.257..  Learning rate: 0.001..\n",
      "Epoch: 227/1500..  Training Loss: -7.139..  Test Loss: -7.056..  Learning rate: 0.001..\n",
      "Epoch: 228/1500..  Training Loss: -7.116..  Test Loss: -7.260..  Learning rate: 0.001..\n",
      "Epoch: 229/1500..  Training Loss: -7.131..  Test Loss: -7.210..  Learning rate: 0.001..\n",
      "Epoch: 230/1500..  Training Loss: -7.126..  Test Loss: -7.192..  Learning rate: 0.001..\n",
      "Epoch: 231/1500..  Training Loss: -7.139..  Test Loss: -7.226..  Learning rate: 0.001..\n",
      "Epoch: 232/1500..  Training Loss: -7.151..  Test Loss: -7.097..  Learning rate: 0.001..\n",
      "Epoch: 233/1500..  Training Loss: -7.046..  Test Loss: -6.897..  Learning rate: 0.001..\n",
      "Epoch: 234/1500..  Training Loss: -7.166..  Test Loss: -7.027..  Learning rate: 0.001..\n",
      "Epoch: 235/1500..  Training Loss: -7.157..  Test Loss: -7.196..  Learning rate: 0.001..\n",
      "Epoch: 236/1500..  Training Loss: -7.120..  Test Loss: -7.172..  Learning rate: 0.001..\n",
      "Epoch: 237/1500..  Training Loss: -7.139..  Test Loss: -6.774..  Learning rate: 0.001..\n",
      "Epoch: 238/1500..  Training Loss: -7.127..  Test Loss: -6.657..  Learning rate: 0.001..\n",
      "Epoch: 239/1500..  Training Loss: -7.130..  Test Loss: -7.208..  Learning rate: 0.001..\n",
      "Epoch: 240/1500..  Training Loss: -7.113..  Test Loss: -7.230..  Learning rate: 0.001..\n",
      "Epoch: 241/1500..  Training Loss: -7.099..  Test Loss: -6.490..  Learning rate: 0.001..\n",
      "Epoch: 242/1500..  Training Loss: -7.160..  Test Loss: -6.942..  Learning rate: 0.001..\n",
      "Epoch: 243/1500..  Training Loss: -7.170..  Test Loss: -7.237..  Learning rate: 0.001..\n",
      "Epoch: 244/1500..  Training Loss: -7.123..  Test Loss: -6.883..  Learning rate: 0.001..\n",
      "Epoch: 245/1500..  Training Loss: -7.123..  Test Loss: -6.984..  Learning rate: 0.001..\n",
      "Epoch: 246/1500..  Training Loss: -7.140..  Test Loss: -6.968..  Learning rate: 0.001..\n",
      "Epoch: 247/1500..  Training Loss: -7.120..  Test Loss: -7.078..  Learning rate: 0.001..\n",
      "Epoch: 248/1500..  Training Loss: -7.224..  Test Loss: -7.350..  Learning rate: 0.001..\n",
      "Validation loss decreased (0.0006740009412169456 --> 0.00064269540598616).  Saving model ...\n",
      "Epoch: 249/1500..  Training Loss: -7.119..  Test Loss: -7.310..  Learning rate: 0.001..\n",
      "Epoch: 250/1500..  Training Loss: -7.127..  Test Loss: -7.212..  Learning rate: 0.001..\n",
      "Epoch: 251/1500..  Training Loss: -7.111..  Test Loss: -7.208..  Learning rate: 0.001..\n",
      "Epoch: 252/1500..  Training Loss: -7.172..  Test Loss: -7.246..  Learning rate: 0.001..\n",
      "Epoch: 253/1500..  Training Loss: -7.190..  Test Loss: -7.270..  Learning rate: 0.001..\n",
      "Epoch: 254/1500..  Training Loss: -7.151..  Test Loss: -7.087..  Learning rate: 0.001..\n",
      "Epoch: 255/1500..  Training Loss: -7.106..  Test Loss: -6.912..  Learning rate: 0.001..\n",
      "Epoch: 256/1500..  Training Loss: -7.143..  Test Loss: -7.343..  Learning rate: 0.001..\n",
      "Epoch: 257/1500..  Training Loss: -7.139..  Test Loss: -7.306..  Learning rate: 0.001..\n",
      "Epoch: 258/1500..  Training Loss: -7.088..  Test Loss: -7.164..  Learning rate: 0.001..\n",
      "Epoch: 259/1500..  Training Loss: -7.176..  Test Loss: -7.139..  Learning rate: 0.001..\n",
      "Epoch: 260/1500..  Training Loss: -7.121..  Test Loss: -7.002..  Learning rate: 0.001..\n",
      "Epoch: 261/1500..  Training Loss: -7.183..  Test Loss: -7.256..  Learning rate: 0.001..\n",
      "Epoch: 262/1500..  Training Loss: -7.086..  Test Loss: -7.131..  Learning rate: 0.001..\n",
      "Epoch: 263/1500..  Training Loss: -7.178..  Test Loss: -7.200..  Learning rate: 0.001..\n",
      "Epoch: 264/1500..  Training Loss: -7.151..  Test Loss: -7.240..  Learning rate: 0.001..\n",
      "Epoch: 265/1500..  Training Loss: -7.219..  Test Loss: -7.183..  Learning rate: 0.001..\n",
      "Epoch: 266/1500..  Training Loss: -7.164..  Test Loss: -7.105..  Learning rate: 0.001..\n",
      "Epoch: 267/1500..  Training Loss: -7.158..  Test Loss: -7.195..  Learning rate: 0.001..\n",
      "Epoch: 268/1500..  Training Loss: -7.178..  Test Loss: -6.844..  Learning rate: 0.001..\n",
      "Epoch: 269/1500..  Training Loss: -7.056..  Test Loss: -7.113..  Learning rate: 0.001..\n",
      "Epoch: 270/1500..  Training Loss: -7.199..  Test Loss: -7.111..  Learning rate: 0.001..\n",
      "Epoch: 271/1500..  Training Loss: -7.159..  Test Loss: -7.342..  Learning rate: 0.001..\n",
      "Epoch: 272/1500..  Training Loss: -7.110..  Test Loss: -7.045..  Learning rate: 0.001..\n",
      "Epoch: 273/1500..  Training Loss: -7.172..  Test Loss: -7.135..  Learning rate: 0.001..\n",
      "Epoch: 274/1500..  Training Loss: -7.122..  Test Loss: -7.167..  Learning rate: 0.001..\n",
      "Epoch: 275/1500..  Training Loss: -7.101..  Test Loss: -7.140..  Learning rate: 0.001..\n",
      "Epoch: 276/1500..  Training Loss: -7.116..  Test Loss: -7.268..  Learning rate: 0.001..\n",
      "Epoch: 277/1500..  Training Loss: -7.125..  Test Loss: -7.127..  Learning rate: 0.001..\n",
      "Epoch: 278/1500..  Training Loss: -7.196..  Test Loss: -7.187..  Learning rate: 0.001..\n",
      "Epoch: 279/1500..  Training Loss: -7.116..  Test Loss: -7.222..  Learning rate: 0.001..\n",
      "Epoch: 280/1500..  Training Loss: -7.139..  Test Loss: -6.908..  Learning rate: 0.001..\n",
      "Epoch: 281/1500..  Training Loss: -7.089..  Test Loss: -6.719..  Learning rate: 0.001..\n",
      "Epoch: 282/1500..  Training Loss: -7.107..  Test Loss: -7.125..  Learning rate: 0.001..\n",
      "Epoch: 283/1500..  Training Loss: -7.176..  Test Loss: -7.074..  Learning rate: 0.001..\n",
      "Epoch: 284/1500..  Training Loss: -7.119..  Test Loss: -7.317..  Learning rate: 0.001..\n",
      "Epoch: 285/1500..  Training Loss: -7.101..  Test Loss: -7.258..  Learning rate: 0.001..\n",
      "Epoch: 286/1500..  Training Loss: -7.128..  Test Loss: -7.255..  Learning rate: 0.001..\n",
      "Epoch: 287/1500..  Training Loss: -7.154..  Test Loss: -7.198..  Learning rate: 0.001..\n",
      "Epoch: 288/1500..  Training Loss: -7.127..  Test Loss: -7.153..  Learning rate: 0.001..\n",
      "Epoch: 289/1500..  Training Loss: -7.145..  Test Loss: -7.269..  Learning rate: 0.001..\n",
      "Epoch: 290/1500..  Training Loss: -7.161..  Test Loss: -7.240..  Learning rate: 0.001..\n",
      "Epoch: 291/1500..  Training Loss: -7.142..  Test Loss: -7.231..  Learning rate: 0.001..\n",
      "Epoch: 292/1500..  Training Loss: -7.154..  Test Loss: -7.263..  Learning rate: 0.001..\n",
      "Epoch: 293/1500..  Training Loss: -7.078..  Test Loss: -7.205..  Learning rate: 0.001..\n",
      "Epoch: 294/1500..  Training Loss: -7.110..  Test Loss: -7.271..  Learning rate: 0.001..\n",
      "Epoch: 295/1500..  Training Loss: -7.191..  Test Loss: -7.343..  Learning rate: 0.001..\n",
      "Epoch: 296/1500..  Training Loss: -7.160..  Test Loss: -7.324..  Learning rate: 0.001..\n",
      "Epoch: 297/1500..  Training Loss: -7.175..  Test Loss: -7.120..  Learning rate: 0.001..\n",
      "Epoch: 298/1500..  Training Loss: -7.171..  Test Loss: -7.236..  Learning rate: 0.001..\n",
      "Epoch: 299/1500..  Training Loss: -7.114..  Test Loss: -7.201..  Learning rate: 0.001..\n",
      "Epoch: 300/1500..  Training Loss: -7.071..  Test Loss: -7.187..  Learning rate: 0.001..\n",
      "Epoch: 301/1500..  Training Loss: -7.170..  Test Loss: -7.273..  Learning rate: 0.001..\n",
      "Epoch: 302/1500..  Training Loss: -7.170..  Test Loss: -7.142..  Learning rate: 0.001..\n",
      "Epoch: 303/1500..  Training Loss: -7.174..  Test Loss: -7.264..  Learning rate: 0.001..\n",
      "Epoch: 304/1500..  Training Loss: -7.056..  Test Loss: -7.278..  Learning rate: 0.001..\n",
      "Epoch: 305/1500..  Training Loss: -7.187..  Test Loss: -6.783..  Learning rate: 0.001..\n",
      "Epoch: 306/1500..  Training Loss: -7.125..  Test Loss: -7.291..  Learning rate: 0.001..\n",
      "Epoch: 307/1500..  Training Loss: -7.181..  Test Loss: -7.051..  Learning rate: 0.001..\n",
      "Epoch: 308/1500..  Training Loss: -7.149..  Test Loss: -6.986..  Learning rate: 0.001..\n",
      "Epoch: 309/1500..  Training Loss: -7.205..  Test Loss: -7.054..  Learning rate: 0.001..\n",
      "Epoch: 310/1500..  Training Loss: -7.126..  Test Loss: -7.121..  Learning rate: 0.001..\n",
      "Epoch: 311/1500..  Training Loss: -7.201..  Test Loss: -7.299..  Learning rate: 0.001..\n",
      "Epoch: 312/1500..  Training Loss: -7.170..  Test Loss: -7.140..  Learning rate: 0.001..\n",
      "Epoch: 313/1500..  Training Loss: -7.160..  Test Loss: -7.206..  Learning rate: 0.001..\n",
      "Epoch: 314/1500..  Training Loss: -7.166..  Test Loss: -7.044..  Learning rate: 0.001..\n",
      "Epoch: 315/1500..  Training Loss: -7.174..  Test Loss: -7.236..  Learning rate: 0.001..\n",
      "Epoch: 316/1500..  Training Loss: -7.142..  Test Loss: -6.740..  Learning rate: 0.001..\n",
      "Epoch: 317/1500..  Training Loss: -7.172..  Test Loss: -7.119..  Learning rate: 0.001..\n",
      "Epoch: 318/1500..  Training Loss: -7.147..  Test Loss: -7.048..  Learning rate: 0.001..\n",
      "Epoch: 319/1500..  Training Loss: -7.106..  Test Loss: -7.233..  Learning rate: 0.001..\n",
      "Epoch: 320/1500..  Training Loss: -7.172..  Test Loss: -7.186..  Learning rate: 0.001..\n",
      "Epoch: 321/1500..  Training Loss: -7.206..  Test Loss: -7.245..  Learning rate: 0.001..\n",
      "Epoch: 322/1500..  Training Loss: -7.139..  Test Loss: -7.238..  Learning rate: 0.001..\n",
      "Epoch: 323/1500..  Training Loss: -7.176..  Test Loss: -7.312..  Learning rate: 0.001..\n",
      "Epoch: 324/1500..  Training Loss: -7.214..  Test Loss: -7.153..  Learning rate: 0.001..\n",
      "Epoch: 325/1500..  Training Loss: -7.133..  Test Loss: -7.074..  Learning rate: 0.001..\n",
      "Epoch: 326/1500..  Training Loss: -7.193..  Test Loss: -7.166..  Learning rate: 0.001..\n",
      "Epoch: 327/1500..  Training Loss: -7.038..  Test Loss: -7.143..  Learning rate: 0.001..\n",
      "Epoch: 328/1500..  Training Loss: -7.167..  Test Loss: -7.225..  Learning rate: 0.001..\n",
      "Epoch: 329/1500..  Training Loss: -7.115..  Test Loss: -7.354..  Learning rate: 0.001..\n",
      "Validation loss decreased (0.00064269540598616 --> 0.0006402604631148279).  Saving model ...\n",
      "Epoch: 330/1500..  Training Loss: -7.146..  Test Loss: -7.281..  Learning rate: 0.001..\n",
      "Epoch: 331/1500..  Training Loss: -7.124..  Test Loss: -7.209..  Learning rate: 0.001..\n",
      "Epoch: 332/1500..  Training Loss: -7.205..  Test Loss: -7.302..  Learning rate: 0.001..\n",
      "Epoch: 333/1500..  Training Loss: -7.200..  Test Loss: -7.258..  Learning rate: 0.001..\n",
      "Epoch: 334/1500..  Training Loss: -7.213..  Test Loss: -7.326..  Learning rate: 0.001..\n",
      "Epoch: 335/1500..  Training Loss: -7.152..  Test Loss: -6.780..  Learning rate: 0.001..\n",
      "Epoch: 336/1500..  Training Loss: -7.100..  Test Loss: -7.233..  Learning rate: 0.001..\n",
      "Epoch: 337/1500..  Training Loss: -7.199..  Test Loss: -7.275..  Learning rate: 0.001..\n",
      "Epoch: 338/1500..  Training Loss: -7.208..  Test Loss: -7.249..  Learning rate: 0.001..\n",
      "Epoch: 339/1500..  Training Loss: -7.158..  Test Loss: -6.889..  Learning rate: 0.001..\n",
      "Epoch: 340/1500..  Training Loss: -7.157..  Test Loss: -7.102..  Learning rate: 0.001..\n",
      "Epoch: 341/1500..  Training Loss: -7.123..  Test Loss: -7.188..  Learning rate: 0.001..\n",
      "Epoch: 342/1500..  Training Loss: -7.102..  Test Loss: -7.065..  Learning rate: 0.001..\n",
      "Epoch: 343/1500..  Training Loss: -7.104..  Test Loss: -7.202..  Learning rate: 0.001..\n",
      "Epoch: 344/1500..  Training Loss: -7.204..  Test Loss: -7.336..  Learning rate: 0.001..\n",
      "Epoch: 345/1500..  Training Loss: -7.198..  Test Loss: -7.163..  Learning rate: 0.001..\n",
      "Epoch: 346/1500..  Training Loss: -7.120..  Test Loss: -6.839..  Learning rate: 0.001..\n",
      "Epoch: 347/1500..  Training Loss: -7.071..  Test Loss: -7.294..  Learning rate: 0.001..\n",
      "Epoch: 348/1500..  Training Loss: -7.179..  Test Loss: -7.230..  Learning rate: 0.001..\n",
      "Epoch: 349/1500..  Training Loss: -7.195..  Test Loss: -7.269..  Learning rate: 0.001..\n",
      "Epoch: 350/1500..  Training Loss: -7.177..  Test Loss: -7.124..  Learning rate: 0.001..\n",
      "Epoch: 351/1500..  Training Loss: -7.208..  Test Loss: -7.274..  Learning rate: 0.001..\n",
      "Epoch: 352/1500..  Training Loss: -7.157..  Test Loss: -7.331..  Learning rate: 0.001..\n",
      "Epoch: 353/1500..  Training Loss: -7.151..  Test Loss: -7.313..  Learning rate: 0.001..\n",
      "Epoch: 354/1500..  Training Loss: -7.145..  Test Loss: -6.873..  Learning rate: 0.001..\n",
      "Epoch: 355/1500..  Training Loss: -7.143..  Test Loss: -7.073..  Learning rate: 0.001..\n",
      "Epoch: 356/1500..  Training Loss: -7.180..  Test Loss: -7.308..  Learning rate: 0.001..\n",
      "Epoch: 357/1500..  Training Loss: -7.067..  Test Loss: -7.276..  Learning rate: 0.001..\n",
      "Epoch: 358/1500..  Training Loss: -7.182..  Test Loss: -7.147..  Learning rate: 0.001..\n",
      "Epoch: 359/1500..  Training Loss: -7.188..  Test Loss: -6.889..  Learning rate: 0.001..\n",
      "Epoch: 360/1500..  Training Loss: -7.170..  Test Loss: -7.287..  Learning rate: 0.001..\n",
      "Epoch: 361/1500..  Training Loss: -7.184..  Test Loss: -6.869..  Learning rate: 0.001..\n",
      "Epoch: 362/1500..  Training Loss: -7.244..  Test Loss: -7.190..  Learning rate: 0.001..\n",
      "Epoch: 363/1500..  Training Loss: -7.102..  Test Loss: -7.204..  Learning rate: 0.001..\n",
      "Epoch: 364/1500..  Training Loss: -7.187..  Test Loss: -7.217..  Learning rate: 0.001..\n",
      "Epoch: 365/1500..  Training Loss: -7.157..  Test Loss: -7.086..  Learning rate: 0.001..\n",
      "Epoch: 366/1500..  Training Loss: -7.169..  Test Loss: -7.149..  Learning rate: 0.001..\n",
      "Epoch: 367/1500..  Training Loss: -7.142..  Test Loss: -7.044..  Learning rate: 0.001..\n",
      "Epoch: 368/1500..  Training Loss: -7.143..  Test Loss: -7.153..  Learning rate: 0.001..\n",
      "Epoch: 369/1500..  Training Loss: -7.165..  Test Loss: -7.148..  Learning rate: 0.001..\n",
      "Epoch: 370/1500..  Training Loss: -7.187..  Test Loss: -7.245..  Learning rate: 0.001..\n",
      "Epoch: 371/1500..  Training Loss: -7.179..  Test Loss: -7.086..  Learning rate: 0.001..\n",
      "Epoch: 372/1500..  Training Loss: -7.195..  Test Loss: -7.074..  Learning rate: 0.001..\n",
      "Epoch: 373/1500..  Training Loss: -7.166..  Test Loss: -7.264..  Learning rate: 0.001..\n",
      "Epoch: 374/1500..  Training Loss: -7.181..  Test Loss: -7.124..  Learning rate: 0.001..\n",
      "Epoch: 375/1500..  Training Loss: -7.171..  Test Loss: -6.788..  Learning rate: 0.001..\n",
      "Epoch: 376/1500..  Training Loss: -7.081..  Test Loss: -7.282..  Learning rate: 0.001..\n",
      "Epoch: 377/1500..  Training Loss: -7.091..  Test Loss: -6.877..  Learning rate: 0.001..\n",
      "Epoch: 378/1500..  Training Loss: -7.133..  Test Loss: -7.311..  Learning rate: 0.001..\n",
      "Epoch: 379/1500..  Training Loss: -7.189..  Test Loss: -7.331..  Learning rate: 0.001..\n",
      "Epoch: 380/1500..  Training Loss: -7.139..  Test Loss: -7.289..  Learning rate: 0.001..\n",
      "Epoch: 381/1500..  Training Loss: -7.170..  Test Loss: -7.306..  Learning rate: 0.001..\n",
      "Epoch: 382/1500..  Training Loss: -7.164..  Test Loss: -7.305..  Learning rate: 0.001..\n",
      "Epoch: 383/1500..  Training Loss: -7.169..  Test Loss: -6.862..  Learning rate: 0.001..\n",
      "Epoch: 384/1500..  Training Loss: -7.133..  Test Loss: -6.977..  Learning rate: 0.001..\n",
      "Epoch: 385/1500..  Training Loss: -7.208..  Test Loss: -7.317..  Learning rate: 0.001..\n",
      "Epoch: 386/1500..  Training Loss: -7.085..  Test Loss: -7.018..  Learning rate: 0.001..\n",
      "Epoch: 387/1500..  Training Loss: -7.181..  Test Loss: -7.197..  Learning rate: 0.001..\n",
      "Epoch: 388/1500..  Training Loss: -7.180..  Test Loss: -7.217..  Learning rate: 0.001..\n",
      "Epoch: 389/1500..  Training Loss: -7.182..  Test Loss: -6.559..  Learning rate: 0.001..\n",
      "Epoch: 390/1500..  Training Loss: -7.195..  Test Loss: -7.293..  Learning rate: 0.001..\n",
      "Epoch: 391/1500..  Training Loss: -7.186..  Test Loss: -7.241..  Learning rate: 0.001..\n",
      "Epoch: 392/1500..  Training Loss: -7.190..  Test Loss: -7.276..  Learning rate: 0.001..\n",
      "Epoch: 393/1500..  Training Loss: -7.150..  Test Loss: -7.303..  Learning rate: 0.001..\n",
      "Epoch: 394/1500..  Training Loss: -7.128..  Test Loss: -7.080..  Learning rate: 0.001..\n",
      "Epoch: 395/1500..  Training Loss: -7.142..  Test Loss: -7.340..  Learning rate: 0.001..\n",
      "Epoch: 396/1500..  Training Loss: -7.185..  Test Loss: -7.106..  Learning rate: 0.001..\n",
      "Epoch: 397/1500..  Training Loss: -7.125..  Test Loss: -7.287..  Learning rate: 0.001..\n",
      "Epoch: 398/1500..  Training Loss: -7.193..  Test Loss: -7.165..  Learning rate: 0.001..\n",
      "Epoch: 399/1500..  Training Loss: -7.177..  Test Loss: -7.328..  Learning rate: 0.001..\n",
      "Epoch: 400/1500..  Training Loss: -7.209..  Test Loss: -7.362..  Learning rate: 0.001..\n",
      "Validation loss decreased (0.0006402604631148279 --> 0.0006349051836878061).  Saving model ...\n",
      "Epoch: 401/1500..  Training Loss: -7.221..  Test Loss: -7.353..  Learning rate: 0.001..\n",
      "Epoch: 402/1500..  Training Loss: -7.117..  Test Loss: -7.265..  Learning rate: 0.001..\n",
      "Epoch: 403/1500..  Training Loss: -7.143..  Test Loss: -7.288..  Learning rate: 0.001..\n",
      "Epoch: 404/1500..  Training Loss: -7.165..  Test Loss: -7.239..  Learning rate: 0.001..\n",
      "Epoch: 405/1500..  Training Loss: -7.170..  Test Loss: -6.947..  Learning rate: 0.001..\n",
      "Epoch: 406/1500..  Training Loss: -7.170..  Test Loss: -7.354..  Learning rate: 0.001..\n",
      "Epoch: 407/1500..  Training Loss: -7.166..  Test Loss: -7.077..  Learning rate: 0.001..\n",
      "Epoch: 408/1500..  Training Loss: -7.204..  Test Loss: -6.871..  Learning rate: 0.001..\n",
      "Epoch: 409/1500..  Training Loss: -7.194..  Test Loss: -7.279..  Learning rate: 0.001..\n",
      "Epoch: 410/1500..  Training Loss: -7.153..  Test Loss: -7.280..  Learning rate: 0.001..\n",
      "Epoch: 411/1500..  Training Loss: -7.200..  Test Loss: -7.334..  Learning rate: 0.001..\n",
      "Epoch: 412/1500..  Training Loss: -7.219..  Test Loss: -7.019..  Learning rate: 0.001..\n",
      "Epoch: 413/1500..  Training Loss: -7.107..  Test Loss: -7.265..  Learning rate: 0.001..\n",
      "Epoch: 414/1500..  Training Loss: -7.083..  Test Loss: -7.307..  Learning rate: 0.001..\n",
      "Epoch: 415/1500..  Training Loss: -7.178..  Test Loss: -7.246..  Learning rate: 0.001..\n",
      "Epoch: 416/1500..  Training Loss: -7.156..  Test Loss: -7.231..  Learning rate: 0.001..\n",
      "Epoch: 417/1500..  Training Loss: -7.173..  Test Loss: -7.198..  Learning rate: 0.001..\n",
      "Epoch: 418/1500..  Training Loss: -7.181..  Test Loss: -6.701..  Learning rate: 0.001..\n",
      "Epoch: 419/1500..  Training Loss: -7.188..  Test Loss: -6.804..  Learning rate: 0.001..\n",
      "Epoch: 420/1500..  Training Loss: -7.214..  Test Loss: -7.197..  Learning rate: 0.001..\n",
      "Epoch: 421/1500..  Training Loss: -7.134..  Test Loss: -7.238..  Learning rate: 0.001..\n",
      "Epoch: 422/1500..  Training Loss: -7.024..  Test Loss: -7.165..  Learning rate: 0.001..\n",
      "Epoch: 423/1500..  Training Loss: -7.112..  Test Loss: -7.189..  Learning rate: 0.001..\n",
      "Epoch: 424/1500..  Training Loss: -7.183..  Test Loss: -7.029..  Learning rate: 0.001..\n",
      "Epoch: 425/1500..  Training Loss: -7.151..  Test Loss: -7.227..  Learning rate: 0.001..\n",
      "Epoch: 426/1500..  Training Loss: -7.132..  Test Loss: -6.663..  Learning rate: 0.001..\n",
      "Epoch: 427/1500..  Training Loss: -7.179..  Test Loss: -7.296..  Learning rate: 0.001..\n",
      "Epoch: 428/1500..  Training Loss: -7.157..  Test Loss: -7.041..  Learning rate: 0.001..\n",
      "Epoch: 429/1500..  Training Loss: -7.181..  Test Loss: -7.279..  Learning rate: 0.001..\n",
      "Epoch: 430/1500..  Training Loss: -7.212..  Test Loss: -7.208..  Learning rate: 0.001..\n",
      "Epoch: 431/1500..  Training Loss: -7.163..  Test Loss: -6.680..  Learning rate: 0.001..\n",
      "Epoch: 432/1500..  Training Loss: -7.138..  Test Loss: -7.048..  Learning rate: 0.001..\n",
      "Epoch: 433/1500..  Training Loss: -7.181..  Test Loss: -7.230..  Learning rate: 0.001..\n",
      "Epoch: 434/1500..  Training Loss: -7.216..  Test Loss: -7.170..  Learning rate: 0.001..\n",
      "Epoch: 435/1500..  Training Loss: -7.209..  Test Loss: -7.361..  Learning rate: 0.001..\n",
      "Epoch: 436/1500..  Training Loss: -7.098..  Test Loss: -7.266..  Learning rate: 0.001..\n",
      "Epoch: 437/1500..  Training Loss: -7.225..  Test Loss: -7.323..  Learning rate: 0.001..\n",
      "Epoch: 438/1500..  Training Loss: -7.171..  Test Loss: -7.304..  Learning rate: 0.001..\n",
      "Epoch: 439/1500..  Training Loss: -7.147..  Test Loss: -7.061..  Learning rate: 0.001..\n",
      "Epoch: 440/1500..  Training Loss: -7.223..  Test Loss: -7.059..  Learning rate: 0.001..\n",
      "Epoch: 441/1500..  Training Loss: -7.151..  Test Loss: -7.068..  Learning rate: 0.001..\n",
      "Epoch: 442/1500..  Training Loss: -7.191..  Test Loss: -7.259..  Learning rate: 0.001..\n",
      "Epoch: 443/1500..  Training Loss: -7.204..  Test Loss: -7.314..  Learning rate: 0.001..\n",
      "Epoch: 444/1500..  Training Loss: -7.190..  Test Loss: -7.294..  Learning rate: 0.001..\n",
      "Epoch: 445/1500..  Training Loss: -7.234..  Test Loss: -7.237..  Learning rate: 0.001..\n",
      "Epoch: 446/1500..  Training Loss: -7.210..  Test Loss: -7.287..  Learning rate: 0.001..\n",
      "Epoch: 447/1500..  Training Loss: -7.187..  Test Loss: -7.177..  Learning rate: 0.001..\n",
      "Epoch: 448/1500..  Training Loss: -7.158..  Test Loss: -6.831..  Learning rate: 0.001..\n",
      "Epoch: 449/1500..  Training Loss: -7.203..  Test Loss: -7.254..  Learning rate: 0.001..\n",
      "Epoch: 450/1500..  Training Loss: -7.224..  Test Loss: -7.209..  Learning rate: 0.001..\n",
      "Epoch: 451/1500..  Training Loss: -7.190..  Test Loss: -6.978..  Learning rate: 0.001..\n",
      "Epoch: 452/1500..  Training Loss: -7.198..  Test Loss: -7.160..  Learning rate: 0.001..\n",
      "Epoch: 453/1500..  Training Loss: -7.190..  Test Loss: -6.921..  Learning rate: 0.001..\n",
      "Epoch: 454/1500..  Training Loss: -7.156..  Test Loss: -7.305..  Learning rate: 0.001..\n",
      "Epoch: 455/1500..  Training Loss: -7.190..  Test Loss: -7.350..  Learning rate: 0.001..\n",
      "Epoch: 456/1500..  Training Loss: -7.149..  Test Loss: -6.886..  Learning rate: 0.001..\n",
      "Epoch: 457/1500..  Training Loss: -7.172..  Test Loss: -7.310..  Learning rate: 0.001..\n",
      "Epoch: 458/1500..  Training Loss: -7.231..  Test Loss: -7.263..  Learning rate: 0.001..\n",
      "Epoch: 459/1500..  Training Loss: -7.160..  Test Loss: -7.294..  Learning rate: 0.001..\n",
      "Epoch: 460/1500..  Training Loss: -7.218..  Test Loss: -7.319..  Learning rate: 0.001..\n",
      "Epoch: 461/1500..  Training Loss: -7.234..  Test Loss: -7.275..  Learning rate: 0.001..\n",
      "Epoch: 462/1500..  Training Loss: -7.196..  Test Loss: -7.242..  Learning rate: 0.001..\n",
      "Epoch: 463/1500..  Training Loss: -7.190..  Test Loss: -7.248..  Learning rate: 0.001..\n",
      "Epoch: 464/1500..  Training Loss: -7.189..  Test Loss: -7.306..  Learning rate: 0.001..\n",
      "Epoch: 465/1500..  Training Loss: -7.235..  Test Loss: -6.864..  Learning rate: 0.001..\n",
      "Epoch: 466/1500..  Training Loss: -7.173..  Test Loss: -7.276..  Learning rate: 0.001..\n",
      "Epoch: 467/1500..  Training Loss: -7.173..  Test Loss: -7.257..  Learning rate: 0.001..\n",
      "Epoch: 468/1500..  Training Loss: -7.201..  Test Loss: -7.290..  Learning rate: 0.001..\n",
      "Epoch: 469/1500..  Training Loss: -7.139..  Test Loss: -7.107..  Learning rate: 0.001..\n",
      "Epoch: 470/1500..  Training Loss: -7.216..  Test Loss: -7.324..  Learning rate: 0.001..\n",
      "Epoch: 471/1500..  Training Loss: -7.207..  Test Loss: -7.275..  Learning rate: 0.001..\n",
      "Epoch: 472/1500..  Training Loss: -7.202..  Test Loss: -6.903..  Learning rate: 0.001..\n",
      "Epoch: 473/1500..  Training Loss: -7.219..  Test Loss: -7.327..  Learning rate: 0.001..\n",
      "Epoch: 474/1500..  Training Loss: -7.200..  Test Loss: -6.969..  Learning rate: 0.001..\n",
      "Epoch: 475/1500..  Training Loss: -7.232..  Test Loss: -7.248..  Learning rate: 0.001..\n",
      "Epoch: 476/1500..  Training Loss: -7.136..  Test Loss: -7.103..  Learning rate: 0.001..\n",
      "Epoch: 477/1500..  Training Loss: -7.156..  Test Loss: -6.895..  Learning rate: 0.001..\n",
      "Epoch: 478/1500..  Training Loss: -7.157..  Test Loss: -7.222..  Learning rate: 0.001..\n",
      "Epoch: 479/1500..  Training Loss: -7.194..  Test Loss: -7.370..  Learning rate: 0.001..\n",
      "Validation loss decreased (0.0006349051836878061 --> 0.0006301724351942539).  Saving model ...\n",
      "Epoch: 480/1500..  Training Loss: -7.196..  Test Loss: -7.288..  Learning rate: 0.001..\n",
      "Epoch: 481/1500..  Training Loss: -7.204..  Test Loss: -7.350..  Learning rate: 0.001..\n",
      "Epoch: 482/1500..  Training Loss: -7.205..  Test Loss: -7.366..  Learning rate: 0.001..\n",
      "Epoch: 483/1500..  Training Loss: -7.211..  Test Loss: -7.041..  Learning rate: 0.001..\n",
      "Epoch: 484/1500..  Training Loss: -7.203..  Test Loss: -7.238..  Learning rate: 0.001..\n",
      "Epoch: 485/1500..  Training Loss: -7.253..  Test Loss: -7.338..  Learning rate: 0.001..\n",
      "Epoch: 486/1500..  Training Loss: -7.167..  Test Loss: -7.129..  Learning rate: 0.001..\n",
      "Epoch: 487/1500..  Training Loss: -7.228..  Test Loss: -7.263..  Learning rate: 0.001..\n",
      "Epoch: 488/1500..  Training Loss: -7.181..  Test Loss: -7.001..  Learning rate: 0.001..\n",
      "Epoch: 489/1500..  Training Loss: -7.154..  Test Loss: -7.235..  Learning rate: 0.001..\n",
      "Epoch: 490/1500..  Training Loss: -7.194..  Test Loss: -7.147..  Learning rate: 0.001..\n",
      "Epoch: 491/1500..  Training Loss: -7.209..  Test Loss: -7.004..  Learning rate: 0.001..\n",
      "Epoch: 492/1500..  Training Loss: -7.220..  Test Loss: -7.162..  Learning rate: 0.001..\n",
      "Epoch: 493/1500..  Training Loss: -7.183..  Test Loss: -7.382..  Learning rate: 0.001..\n",
      "Validation loss decreased (0.0006301724351942539 --> 0.0006226020632311702).  Saving model ...\n",
      "Epoch: 494/1500..  Training Loss: -7.135..  Test Loss: -7.189..  Learning rate: 0.001..\n",
      "Epoch: 495/1500..  Training Loss: -7.214..  Test Loss: -7.335..  Learning rate: 0.001..\n",
      "Epoch: 496/1500..  Training Loss: -7.208..  Test Loss: -7.220..  Learning rate: 0.001..\n",
      "Epoch: 497/1500..  Training Loss: -7.156..  Test Loss: -6.940..  Learning rate: 0.001..\n",
      "Epoch: 498/1500..  Training Loss: -7.202..  Test Loss: -6.709..  Learning rate: 0.001..\n",
      "Epoch: 499/1500..  Training Loss: -7.203..  Test Loss: -7.251..  Learning rate: 0.001..\n",
      "Epoch: 500/1500..  Training Loss: -7.175..  Test Loss: -7.247..  Learning rate: 0.0005..\n",
      "Epoch: 501/1500..  Training Loss: -7.389..  Test Loss: -7.403..  Learning rate: 0.0005..\n",
      "Validation loss decreased (0.0006226020632311702 --> 0.0006097032455727458).  Saving model ...\n",
      "Epoch: 502/1500..  Training Loss: -7.320..  Test Loss: -7.427..  Learning rate: 0.0005..\n",
      "Validation loss decreased (0.0006097032455727458 --> 0.0005948848556727171).  Saving model ...\n",
      "Epoch: 503/1500..  Training Loss: -7.316..  Test Loss: -7.389..  Learning rate: 0.0005..\n",
      "Epoch: 504/1500..  Training Loss: -7.366..  Test Loss: -6.909..  Learning rate: 0.0005..\n",
      "Epoch: 505/1500..  Training Loss: -7.327..  Test Loss: -7.384..  Learning rate: 0.0005..\n",
      "Epoch: 506/1500..  Training Loss: -7.342..  Test Loss: -7.428..  Learning rate: 0.0005..\n",
      "Validation loss decreased (0.0005948848556727171 --> 0.0005945486482232809).  Saving model ...\n",
      "Epoch: 507/1500..  Training Loss: -7.382..  Test Loss: -7.410..  Learning rate: 0.0005..\n",
      "Epoch: 508/1500..  Training Loss: -7.368..  Test Loss: -7.377..  Learning rate: 0.0005..\n",
      "Epoch: 509/1500..  Training Loss: -7.363..  Test Loss: -7.219..  Learning rate: 0.0005..\n",
      "Epoch: 510/1500..  Training Loss: -7.342..  Test Loss: -7.443..  Learning rate: 0.0005..\n",
      "Validation loss decreased (0.0005945486482232809 --> 0.0005854283808730543).  Saving model ...\n",
      "Epoch: 511/1500..  Training Loss: -7.331..  Test Loss: -7.369..  Learning rate: 0.0005..\n",
      "Epoch: 512/1500..  Training Loss: -7.389..  Test Loss: -7.446..  Learning rate: 0.0005..\n",
      "Validation loss decreased (0.0005854283808730543 --> 0.0005838582874275744).  Saving model ...\n",
      "Epoch: 513/1500..  Training Loss: -7.333..  Test Loss: -7.334..  Learning rate: 0.0005..\n",
      "Epoch: 514/1500..  Training Loss: -7.372..  Test Loss: -7.311..  Learning rate: 0.0005..\n",
      "Epoch: 515/1500..  Training Loss: -7.310..  Test Loss: -7.380..  Learning rate: 0.0005..\n",
      "Epoch: 516/1500..  Training Loss: -7.257..  Test Loss: -7.209..  Learning rate: 0.0005..\n",
      "Epoch: 517/1500..  Training Loss: -7.350..  Test Loss: -7.394..  Learning rate: 0.0005..\n",
      "Epoch: 518/1500..  Training Loss: -7.339..  Test Loss: -7.409..  Learning rate: 0.0005..\n",
      "Epoch: 519/1500..  Training Loss: -7.349..  Test Loss: -7.358..  Learning rate: 0.0005..\n",
      "Epoch: 520/1500..  Training Loss: -7.368..  Test Loss: -7.402..  Learning rate: 0.0005..\n",
      "Epoch: 521/1500..  Training Loss: -7.351..  Test Loss: -6.871..  Learning rate: 0.0005..\n",
      "Epoch: 522/1500..  Training Loss: -7.333..  Test Loss: -7.324..  Learning rate: 0.0005..\n",
      "Epoch: 523/1500..  Training Loss: -7.385..  Test Loss: -7.397..  Learning rate: 0.0005..\n",
      "Epoch: 524/1500..  Training Loss: -7.326..  Test Loss: -7.450..  Learning rate: 0.0005..\n",
      "Validation loss decreased (0.0005838582874275744 --> 0.0005817102501168847).  Saving model ...\n",
      "Epoch: 525/1500..  Training Loss: -7.321..  Test Loss: -7.434..  Learning rate: 0.0005..\n",
      "Epoch: 526/1500..  Training Loss: -7.383..  Test Loss: -7.407..  Learning rate: 0.0005..\n",
      "Epoch: 527/1500..  Training Loss: -7.376..  Test Loss: -7.414..  Learning rate: 0.0005..\n",
      "Epoch: 528/1500..  Training Loss: -7.355..  Test Loss: -6.939..  Learning rate: 0.0005..\n",
      "Epoch: 529/1500..  Training Loss: -7.348..  Test Loss: -7.312..  Learning rate: 0.0005..\n",
      "Epoch: 530/1500..  Training Loss: -7.355..  Test Loss: -7.381..  Learning rate: 0.0005..\n",
      "Epoch: 531/1500..  Training Loss: -7.382..  Test Loss: -7.452..  Learning rate: 0.0005..\n",
      "Validation loss decreased (0.0005817102501168847 --> 0.0005804412066936493).  Saving model ...\n",
      "Epoch: 532/1500..  Training Loss: -7.379..  Test Loss: -7.398..  Learning rate: 0.0005..\n",
      "Epoch: 533/1500..  Training Loss: -7.384..  Test Loss: -7.292..  Learning rate: 0.0005..\n",
      "Epoch: 534/1500..  Training Loss: -7.310..  Test Loss: -7.437..  Learning rate: 0.0005..\n",
      "Epoch: 535/1500..  Training Loss: -7.400..  Test Loss: -7.432..  Learning rate: 0.0005..\n",
      "Epoch: 536/1500..  Training Loss: -7.291..  Test Loss: -6.773..  Learning rate: 0.0005..\n",
      "Epoch: 537/1500..  Training Loss: -7.356..  Test Loss: -7.278..  Learning rate: 0.0005..\n",
      "Epoch: 538/1500..  Training Loss: -7.377..  Test Loss: -7.280..  Learning rate: 0.0005..\n",
      "Epoch: 539/1500..  Training Loss: -7.286..  Test Loss: -7.400..  Learning rate: 0.0005..\n",
      "Epoch: 540/1500..  Training Loss: -7.364..  Test Loss: -7.275..  Learning rate: 0.0005..\n",
      "Epoch: 541/1500..  Training Loss: -7.378..  Test Loss: -7.415..  Learning rate: 0.0005..\n",
      "Epoch: 542/1500..  Training Loss: -7.369..  Test Loss: -7.164..  Learning rate: 0.0005..\n",
      "Epoch: 543/1500..  Training Loss: -7.371..  Test Loss: -7.159..  Learning rate: 0.0005..\n",
      "Epoch: 544/1500..  Training Loss: -7.260..  Test Loss: -7.415..  Learning rate: 0.0005..\n",
      "Epoch: 545/1500..  Training Loss: -7.306..  Test Loss: -7.426..  Learning rate: 0.0005..\n",
      "Epoch: 546/1500..  Training Loss: -7.316..  Test Loss: -7.427..  Learning rate: 0.0005..\n",
      "Epoch: 547/1500..  Training Loss: -7.357..  Test Loss: -7.385..  Learning rate: 0.0005..\n",
      "Epoch: 548/1500..  Training Loss: -7.322..  Test Loss: -7.397..  Learning rate: 0.0005..\n",
      "Epoch: 549/1500..  Training Loss: -7.350..  Test Loss: -7.340..  Learning rate: 0.0005..\n",
      "Epoch: 550/1500..  Training Loss: -7.356..  Test Loss: -7.314..  Learning rate: 0.0005..\n",
      "Epoch: 551/1500..  Training Loss: -7.296..  Test Loss: -6.908..  Learning rate: 0.0005..\n",
      "Epoch: 552/1500..  Training Loss: -7.217..  Test Loss: -7.229..  Learning rate: 0.0005..\n",
      "Epoch: 553/1500..  Training Loss: -7.345..  Test Loss: -7.247..  Learning rate: 0.0005..\n",
      "Epoch: 554/1500..  Training Loss: -7.339..  Test Loss: -7.406..  Learning rate: 0.0005..\n",
      "Epoch: 555/1500..  Training Loss: -7.385..  Test Loss: -7.274..  Learning rate: 0.0005..\n",
      "Epoch: 556/1500..  Training Loss: -7.312..  Test Loss: -7.303..  Learning rate: 0.0005..\n",
      "Epoch: 557/1500..  Training Loss: -7.388..  Test Loss: -7.431..  Learning rate: 0.0005..\n",
      "Epoch: 558/1500..  Training Loss: -7.338..  Test Loss: -7.243..  Learning rate: 0.0005..\n",
      "Epoch: 559/1500..  Training Loss: -7.314..  Test Loss: -7.382..  Learning rate: 0.0005..\n",
      "Epoch: 560/1500..  Training Loss: -7.360..  Test Loss: -7.393..  Learning rate: 0.0005..\n",
      "Epoch: 561/1500..  Training Loss: -7.389..  Test Loss: -7.374..  Learning rate: 0.0005..\n",
      "Epoch: 562/1500..  Training Loss: -7.361..  Test Loss: -7.436..  Learning rate: 0.0005..\n",
      "Epoch: 563/1500..  Training Loss: -7.363..  Test Loss: -7.225..  Learning rate: 0.0005..\n",
      "Epoch: 564/1500..  Training Loss: -7.286..  Test Loss: -7.164..  Learning rate: 0.0005..\n",
      "Epoch: 565/1500..  Training Loss: -7.350..  Test Loss: -7.349..  Learning rate: 0.0005..\n",
      "Epoch: 566/1500..  Training Loss: -7.328..  Test Loss: -7.418..  Learning rate: 0.0005..\n",
      "Epoch: 567/1500..  Training Loss: -7.296..  Test Loss: -7.421..  Learning rate: 0.0005..\n",
      "Epoch: 568/1500..  Training Loss: -7.313..  Test Loss: -7.400..  Learning rate: 0.0005..\n",
      "Epoch: 569/1500..  Training Loss: -7.291..  Test Loss: -7.397..  Learning rate: 0.0005..\n",
      "Epoch: 570/1500..  Training Loss: -7.319..  Test Loss: -7.190..  Learning rate: 0.0005..\n",
      "Epoch: 571/1500..  Training Loss: -7.266..  Test Loss: -7.213..  Learning rate: 0.0005..\n",
      "Epoch: 572/1500..  Training Loss: -7.326..  Test Loss: -7.391..  Learning rate: 0.0005..\n",
      "Epoch: 573/1500..  Training Loss: -7.354..  Test Loss: -7.360..  Learning rate: 0.0005..\n",
      "Epoch: 574/1500..  Training Loss: -7.292..  Test Loss: -7.419..  Learning rate: 0.0005..\n",
      "Epoch: 575/1500..  Training Loss: -7.297..  Test Loss: -7.365..  Learning rate: 0.0005..\n",
      "Epoch: 576/1500..  Training Loss: -7.314..  Test Loss: -7.354..  Learning rate: 0.0005..\n",
      "Epoch: 577/1500..  Training Loss: -7.371..  Test Loss: -7.333..  Learning rate: 0.0005..\n",
      "Epoch: 578/1500..  Training Loss: -7.356..  Test Loss: -7.201..  Learning rate: 0.0005..\n",
      "Epoch: 579/1500..  Training Loss: -7.339..  Test Loss: -7.170..  Learning rate: 0.0005..\n",
      "Epoch: 580/1500..  Training Loss: -7.295..  Test Loss: -7.082..  Learning rate: 0.0005..\n",
      "Epoch: 581/1500..  Training Loss: -7.350..  Test Loss: -7.306..  Learning rate: 0.0005..\n",
      "Epoch: 582/1500..  Training Loss: -7.363..  Test Loss: -7.325..  Learning rate: 0.0005..\n",
      "Epoch: 583/1500..  Training Loss: -7.339..  Test Loss: -7.368..  Learning rate: 0.0005..\n",
      "Epoch: 584/1500..  Training Loss: -7.362..  Test Loss: -7.353..  Learning rate: 0.0005..\n",
      "Epoch: 585/1500..  Training Loss: -7.366..  Test Loss: -7.424..  Learning rate: 0.0005..\n",
      "Epoch: 586/1500..  Training Loss: -7.317..  Test Loss: -7.428..  Learning rate: 0.0005..\n",
      "Epoch: 587/1500..  Training Loss: -7.334..  Test Loss: -7.374..  Learning rate: 0.0005..\n",
      "Epoch: 588/1500..  Training Loss: -7.332..  Test Loss: -6.967..  Learning rate: 0.0005..\n",
      "Epoch: 589/1500..  Training Loss: -7.262..  Test Loss: -7.414..  Learning rate: 0.0005..\n",
      "Epoch: 590/1500..  Training Loss: -7.225..  Test Loss: -7.097..  Learning rate: 0.0005..\n",
      "Epoch: 591/1500..  Training Loss: -7.308..  Test Loss: -7.391..  Learning rate: 0.0005..\n",
      "Epoch: 592/1500..  Training Loss: -7.351..  Test Loss: -7.342..  Learning rate: 0.0005..\n",
      "Epoch: 593/1500..  Training Loss: -7.380..  Test Loss: -7.433..  Learning rate: 0.0005..\n",
      "Epoch: 594/1500..  Training Loss: -7.347..  Test Loss: -7.193..  Learning rate: 0.0005..\n",
      "Epoch: 595/1500..  Training Loss: -7.370..  Test Loss: -7.458..  Learning rate: 0.0005..\n",
      "Validation loss decreased (0.0005804412066936493 --> 0.0005768304108642042).  Saving model ...\n",
      "Epoch: 596/1500..  Training Loss: -7.298..  Test Loss: -7.434..  Learning rate: 0.0005..\n",
      "Epoch: 597/1500..  Training Loss: -7.342..  Test Loss: -7.420..  Learning rate: 0.0005..\n",
      "Epoch: 598/1500..  Training Loss: -7.316..  Test Loss: -7.399..  Learning rate: 0.0005..\n",
      "Epoch: 599/1500..  Training Loss: -7.359..  Test Loss: -7.022..  Learning rate: 0.0005..\n",
      "Epoch: 600/1500..  Training Loss: -7.338..  Test Loss: -7.335..  Learning rate: 0.0005..\n",
      "Epoch: 601/1500..  Training Loss: -7.355..  Test Loss: -7.424..  Learning rate: 0.0005..\n",
      "Epoch: 602/1500..  Training Loss: -7.329..  Test Loss: -7.367..  Learning rate: 0.0005..\n",
      "Epoch: 603/1500..  Training Loss: -7.219..  Test Loss: -7.213..  Learning rate: 0.0005..\n",
      "Epoch: 604/1500..  Training Loss: -7.314..  Test Loss: -7.382..  Learning rate: 0.0005..\n",
      "Epoch: 605/1500..  Training Loss: -7.332..  Test Loss: -7.363..  Learning rate: 0.0005..\n",
      "Epoch: 606/1500..  Training Loss: -7.335..  Test Loss: -7.412..  Learning rate: 0.0005..\n",
      "Epoch: 607/1500..  Training Loss: -7.331..  Test Loss: -7.331..  Learning rate: 0.0005..\n",
      "Epoch: 608/1500..  Training Loss: -7.331..  Test Loss: -7.359..  Learning rate: 0.0005..\n",
      "Epoch: 609/1500..  Training Loss: -7.332..  Test Loss: -7.414..  Learning rate: 0.0005..\n",
      "Epoch: 610/1500..  Training Loss: -7.313..  Test Loss: -6.841..  Learning rate: 0.0005..\n",
      "Epoch: 611/1500..  Training Loss: -7.295..  Test Loss: -7.333..  Learning rate: 0.0005..\n",
      "Epoch: 612/1500..  Training Loss: -7.402..  Test Loss: -7.295..  Learning rate: 0.0005..\n",
      "Epoch: 613/1500..  Training Loss: -7.354..  Test Loss: -7.343..  Learning rate: 0.0005..\n",
      "Epoch: 614/1500..  Training Loss: -7.304..  Test Loss: -7.363..  Learning rate: 0.0005..\n",
      "Epoch: 615/1500..  Training Loss: -7.350..  Test Loss: -7.365..  Learning rate: 0.0005..\n",
      "Epoch: 616/1500..  Training Loss: -7.369..  Test Loss: -7.469..  Learning rate: 0.0005..\n",
      "Validation loss decreased (0.0005768304108642042 --> 0.0005705107469111681).  Saving model ...\n",
      "Epoch: 617/1500..  Training Loss: -7.400..  Test Loss: -7.426..  Learning rate: 0.0005..\n",
      "Epoch: 618/1500..  Training Loss: -7.200..  Test Loss: -7.373..  Learning rate: 0.0005..\n",
      "Epoch: 619/1500..  Training Loss: -7.379..  Test Loss: -7.396..  Learning rate: 0.0005..\n",
      "Epoch: 620/1500..  Training Loss: -7.293..  Test Loss: -7.391..  Learning rate: 0.0005..\n",
      "Epoch: 621/1500..  Training Loss: -7.313..  Test Loss: -7.214..  Learning rate: 0.0005..\n",
      "Epoch: 622/1500..  Training Loss: -7.351..  Test Loss: -7.422..  Learning rate: 0.0005..\n",
      "Epoch: 623/1500..  Training Loss: -7.372..  Test Loss: -7.260..  Learning rate: 0.0005..\n",
      "Epoch: 624/1500..  Training Loss: -7.318..  Test Loss: -7.364..  Learning rate: 0.0005..\n",
      "Epoch: 625/1500..  Training Loss: -7.371..  Test Loss: -7.380..  Learning rate: 0.0005..\n",
      "Epoch: 626/1500..  Training Loss: -7.360..  Test Loss: -6.845..  Learning rate: 0.0005..\n",
      "Epoch: 627/1500..  Training Loss: -7.285..  Test Loss: -7.387..  Learning rate: 0.0005..\n",
      "Epoch: 628/1500..  Training Loss: -7.365..  Test Loss: -7.453..  Learning rate: 0.0005..\n",
      "Epoch: 629/1500..  Training Loss: -7.334..  Test Loss: -7.341..  Learning rate: 0.0005..\n",
      "Epoch: 630/1500..  Training Loss: -7.339..  Test Loss: -7.366..  Learning rate: 0.0005..\n",
      "Epoch: 631/1500..  Training Loss: -7.338..  Test Loss: -7.369..  Learning rate: 0.0005..\n",
      "Epoch: 632/1500..  Training Loss: -7.370..  Test Loss: -7.334..  Learning rate: 0.0005..\n",
      "Epoch: 633/1500..  Training Loss: -7.364..  Test Loss: -7.400..  Learning rate: 0.0005..\n",
      "Epoch: 634/1500..  Training Loss: -7.366..  Test Loss: -7.082..  Learning rate: 0.0005..\n",
      "Epoch: 635/1500..  Training Loss: -7.340..  Test Loss: -7.285..  Learning rate: 0.0005..\n",
      "Epoch: 636/1500..  Training Loss: -7.349..  Test Loss: -7.434..  Learning rate: 0.0005..\n",
      "Epoch: 637/1500..  Training Loss: -7.320..  Test Loss: -7.407..  Learning rate: 0.0005..\n",
      "Epoch: 638/1500..  Training Loss: -7.360..  Test Loss: -7.309..  Learning rate: 0.0005..\n",
      "Epoch: 639/1500..  Training Loss: -7.331..  Test Loss: -7.453..  Learning rate: 0.0005..\n",
      "Epoch: 640/1500..  Training Loss: -7.223..  Test Loss: -7.334..  Learning rate: 0.0005..\n",
      "Epoch: 641/1500..  Training Loss: -7.367..  Test Loss: -6.937..  Learning rate: 0.0005..\n",
      "Epoch: 642/1500..  Training Loss: -7.309..  Test Loss: -7.404..  Learning rate: 0.0005..\n",
      "Epoch: 643/1500..  Training Loss: -7.303..  Test Loss: -7.347..  Learning rate: 0.0005..\n",
      "Epoch: 644/1500..  Training Loss: -7.366..  Test Loss: -7.299..  Learning rate: 0.0005..\n",
      "Epoch: 645/1500..  Training Loss: -7.351..  Test Loss: -7.403..  Learning rate: 0.0005..\n",
      "Epoch: 646/1500..  Training Loss: -7.249..  Test Loss: -7.278..  Learning rate: 0.0005..\n",
      "Epoch: 647/1500..  Training Loss: -7.323..  Test Loss: -7.337..  Learning rate: 0.0005..\n",
      "Epoch: 648/1500..  Training Loss: -7.393..  Test Loss: -7.287..  Learning rate: 0.0005..\n",
      "Epoch: 649/1500..  Training Loss: -7.255..  Test Loss: -7.424..  Learning rate: 0.0005..\n",
      "Epoch: 650/1500..  Training Loss: -7.310..  Test Loss: -7.359..  Learning rate: 0.0005..\n",
      "Epoch: 651/1500..  Training Loss: -7.303..  Test Loss: -7.309..  Learning rate: 0.0005..\n",
      "Epoch: 652/1500..  Training Loss: -7.388..  Test Loss: -7.385..  Learning rate: 0.0005..\n",
      "Epoch: 653/1500..  Training Loss: -7.106..  Test Loss: -7.360..  Learning rate: 0.0005..\n",
      "Epoch: 654/1500..  Training Loss: -7.341..  Test Loss: -7.146..  Learning rate: 0.0005..\n",
      "Epoch: 655/1500..  Training Loss: -7.295..  Test Loss: -7.392..  Learning rate: 0.0005..\n",
      "Epoch: 656/1500..  Training Loss: -7.389..  Test Loss: -7.359..  Learning rate: 0.0005..\n",
      "Epoch: 657/1500..  Training Loss: -7.313..  Test Loss: -7.377..  Learning rate: 0.0005..\n",
      "Epoch: 658/1500..  Training Loss: -7.278..  Test Loss: -7.403..  Learning rate: 0.0005..\n",
      "Epoch: 659/1500..  Training Loss: -7.308..  Test Loss: -7.404..  Learning rate: 0.0005..\n",
      "Epoch: 660/1500..  Training Loss: -7.332..  Test Loss: -7.442..  Learning rate: 0.0005..\n",
      "Epoch: 661/1500..  Training Loss: -7.329..  Test Loss: -7.434..  Learning rate: 0.0005..\n",
      "Epoch: 662/1500..  Training Loss: -7.335..  Test Loss: -7.362..  Learning rate: 0.0005..\n",
      "Epoch: 663/1500..  Training Loss: -7.294..  Test Loss: -7.380..  Learning rate: 0.0005..\n",
      "Epoch: 664/1500..  Training Loss: -7.356..  Test Loss: -7.364..  Learning rate: 0.0005..\n",
      "Epoch: 665/1500..  Training Loss: -7.318..  Test Loss: -7.398..  Learning rate: 0.0005..\n",
      "Epoch: 666/1500..  Training Loss: -7.328..  Test Loss: -7.405..  Learning rate: 0.0005..\n",
      "Epoch: 667/1500..  Training Loss: -7.331..  Test Loss: -7.286..  Learning rate: 0.0005..\n",
      "Epoch: 668/1500..  Training Loss: -7.316..  Test Loss: -7.391..  Learning rate: 0.0005..\n",
      "Epoch: 669/1500..  Training Loss: -7.363..  Test Loss: -7.321..  Learning rate: 0.0005..\n",
      "Epoch: 670/1500..  Training Loss: -7.320..  Test Loss: -7.407..  Learning rate: 0.0005..\n",
      "Epoch: 671/1500..  Training Loss: -7.380..  Test Loss: -7.324..  Learning rate: 0.0005..\n",
      "Epoch: 672/1500..  Training Loss: -7.332..  Test Loss: -7.318..  Learning rate: 0.0005..\n",
      "Epoch: 673/1500..  Training Loss: -7.383..  Test Loss: -7.418..  Learning rate: 0.0005..\n",
      "Epoch: 674/1500..  Training Loss: -7.390..  Test Loss: -7.351..  Learning rate: 0.0005..\n",
      "Epoch: 675/1500..  Training Loss: -7.314..  Test Loss: -7.281..  Learning rate: 0.0005..\n",
      "Epoch: 676/1500..  Training Loss: -7.322..  Test Loss: -7.355..  Learning rate: 0.0005..\n",
      "Epoch: 677/1500..  Training Loss: -7.280..  Test Loss: -7.407..  Learning rate: 0.0005..\n",
      "Epoch: 678/1500..  Training Loss: -7.346..  Test Loss: -7.225..  Learning rate: 0.0005..\n",
      "Epoch: 679/1500..  Training Loss: -7.396..  Test Loss: -7.325..  Learning rate: 0.0005..\n",
      "Epoch: 680/1500..  Training Loss: -7.370..  Test Loss: -7.356..  Learning rate: 0.0005..\n",
      "Epoch: 681/1500..  Training Loss: -7.334..  Test Loss: -7.229..  Learning rate: 0.0005..\n",
      "Epoch: 682/1500..  Training Loss: -7.307..  Test Loss: -7.245..  Learning rate: 0.0005..\n",
      "Epoch: 683/1500..  Training Loss: -7.364..  Test Loss: -7.072..  Learning rate: 0.0005..\n",
      "Epoch: 684/1500..  Training Loss: -7.357..  Test Loss: -7.237..  Learning rate: 0.0005..\n",
      "Epoch: 685/1500..  Training Loss: -7.382..  Test Loss: -7.401..  Learning rate: 0.0005..\n",
      "Epoch: 686/1500..  Training Loss: -7.377..  Test Loss: -7.462..  Learning rate: 0.0005..\n",
      "Epoch: 687/1500..  Training Loss: -7.352..  Test Loss: -6.869..  Learning rate: 0.0005..\n",
      "Epoch: 688/1500..  Training Loss: -7.375..  Test Loss: -7.352..  Learning rate: 0.0005..\n",
      "Epoch: 689/1500..  Training Loss: -7.332..  Test Loss: -7.287..  Learning rate: 0.0005..\n",
      "Epoch: 690/1500..  Training Loss: -7.370..  Test Loss: -7.309..  Learning rate: 0.0005..\n",
      "Epoch: 691/1500..  Training Loss: -7.273..  Test Loss: -7.372..  Learning rate: 0.0005..\n",
      "Epoch: 692/1500..  Training Loss: -7.289..  Test Loss: -7.290..  Learning rate: 0.0005..\n",
      "Epoch: 693/1500..  Training Loss: -7.377..  Test Loss: -7.416..  Learning rate: 0.0005..\n",
      "Epoch: 694/1500..  Training Loss: -7.310..  Test Loss: -7.401..  Learning rate: 0.0005..\n",
      "Epoch: 695/1500..  Training Loss: -7.375..  Test Loss: -7.399..  Learning rate: 0.0005..\n",
      "Epoch: 696/1500..  Training Loss: -7.333..  Test Loss: -7.025..  Learning rate: 0.0005..\n",
      "Epoch: 697/1500..  Training Loss: -7.298..  Test Loss: -7.369..  Learning rate: 0.0005..\n",
      "Epoch: 698/1500..  Training Loss: -7.369..  Test Loss: -7.351..  Learning rate: 0.0005..\n",
      "Epoch: 699/1500..  Training Loss: -7.302..  Test Loss: -7.370..  Learning rate: 0.0005..\n",
      "Epoch: 700/1500..  Training Loss: -7.354..  Test Loss: -7.439..  Learning rate: 0.0005..\n",
      "Epoch: 701/1500..  Training Loss: -7.306..  Test Loss: -7.232..  Learning rate: 0.0005..\n",
      "Epoch: 702/1500..  Training Loss: -7.332..  Test Loss: -7.381..  Learning rate: 0.0005..\n",
      "Epoch: 703/1500..  Training Loss: -7.366..  Test Loss: -7.373..  Learning rate: 0.0005..\n",
      "Epoch: 704/1500..  Training Loss: -7.341..  Test Loss: -7.190..  Learning rate: 0.0005..\n",
      "Epoch: 705/1500..  Training Loss: -7.397..  Test Loss: -7.426..  Learning rate: 0.0005..\n",
      "Epoch: 706/1500..  Training Loss: -7.410..  Test Loss: -7.100..  Learning rate: 0.0005..\n",
      "Epoch: 707/1500..  Training Loss: -7.373..  Test Loss: -7.419..  Learning rate: 0.0005..\n",
      "Epoch: 708/1500..  Training Loss: -7.315..  Test Loss: -6.884..  Learning rate: 0.0005..\n",
      "Epoch: 709/1500..  Training Loss: -7.326..  Test Loss: -7.167..  Learning rate: 0.0005..\n",
      "Epoch: 710/1500..  Training Loss: -7.293..  Test Loss: -7.262..  Learning rate: 0.0005..\n",
      "Epoch: 711/1500..  Training Loss: -7.409..  Test Loss: -7.376..  Learning rate: 0.0005..\n",
      "Epoch: 712/1500..  Training Loss: -7.333..  Test Loss: -7.379..  Learning rate: 0.0005..\n",
      "Epoch: 713/1500..  Training Loss: -7.378..  Test Loss: -7.405..  Learning rate: 0.0005..\n",
      "Epoch: 714/1500..  Training Loss: -7.296..  Test Loss: -6.896..  Learning rate: 0.0005..\n",
      "Epoch: 715/1500..  Training Loss: -7.357..  Test Loss: -7.416..  Learning rate: 0.0005..\n",
      "Epoch: 716/1500..  Training Loss: -7.310..  Test Loss: -7.437..  Learning rate: 0.0005..\n",
      "Epoch: 717/1500..  Training Loss: -7.315..  Test Loss: -7.434..  Learning rate: 0.0005..\n",
      "Epoch: 718/1500..  Training Loss: -7.224..  Test Loss: -6.883..  Learning rate: 0.0005..\n",
      "Epoch: 719/1500..  Training Loss: -7.350..  Test Loss: -7.393..  Learning rate: 0.0005..\n",
      "Epoch: 720/1500..  Training Loss: -7.274..  Test Loss: -7.342..  Learning rate: 0.0005..\n",
      "Epoch: 721/1500..  Training Loss: -7.316..  Test Loss: -7.403..  Learning rate: 0.0005..\n",
      "Epoch: 722/1500..  Training Loss: -7.308..  Test Loss: -7.442..  Learning rate: 0.0005..\n",
      "Epoch: 723/1500..  Training Loss: -7.336..  Test Loss: -7.037..  Learning rate: 0.0005..\n",
      "Epoch: 724/1500..  Training Loss: -7.385..  Test Loss: -7.427..  Learning rate: 0.0005..\n",
      "Epoch: 725/1500..  Training Loss: -7.300..  Test Loss: -7.156..  Learning rate: 0.0005..\n",
      "Epoch: 726/1500..  Training Loss: -7.371..  Test Loss: -7.395..  Learning rate: 0.0005..\n",
      "Epoch: 727/1500..  Training Loss: -7.327..  Test Loss: -7.259..  Learning rate: 0.0005..\n",
      "Epoch: 728/1500..  Training Loss: -7.346..  Test Loss: -6.985..  Learning rate: 0.0005..\n",
      "Epoch: 729/1500..  Training Loss: -7.351..  Test Loss: -7.063..  Learning rate: 0.0005..\n",
      "Epoch: 730/1500..  Training Loss: -7.339..  Test Loss: -7.394..  Learning rate: 0.0005..\n",
      "Epoch: 731/1500..  Training Loss: -7.366..  Test Loss: -7.321..  Learning rate: 0.0005..\n",
      "Epoch: 732/1500..  Training Loss: -7.298..  Test Loss: -7.423..  Learning rate: 0.0005..\n",
      "Epoch: 733/1500..  Training Loss: -7.362..  Test Loss: -7.382..  Learning rate: 0.0005..\n",
      "Epoch: 734/1500..  Training Loss: -7.284..  Test Loss: -7.361..  Learning rate: 0.0005..\n",
      "Epoch: 735/1500..  Training Loss: -7.334..  Test Loss: -7.351..  Learning rate: 0.0005..\n",
      "Epoch: 736/1500..  Training Loss: -7.277..  Test Loss: -6.780..  Learning rate: 0.0005..\n",
      "Epoch: 737/1500..  Training Loss: -7.299..  Test Loss: -7.291..  Learning rate: 0.0005..\n",
      "Epoch: 738/1500..  Training Loss: -7.352..  Test Loss: -7.391..  Learning rate: 0.0005..\n",
      "Epoch: 739/1500..  Training Loss: -7.324..  Test Loss: -7.379..  Learning rate: 0.0005..\n",
      "Epoch: 740/1500..  Training Loss: -7.390..  Test Loss: -7.223..  Learning rate: 0.0005..\n",
      "Epoch: 741/1500..  Training Loss: -7.358..  Test Loss: -7.422..  Learning rate: 0.0005..\n",
      "Epoch: 742/1500..  Training Loss: -7.373..  Test Loss: -7.383..  Learning rate: 0.0005..\n",
      "Epoch: 743/1500..  Training Loss: -7.332..  Test Loss: -7.385..  Learning rate: 0.0005..\n",
      "Epoch: 744/1500..  Training Loss: -7.400..  Test Loss: -7.412..  Learning rate: 0.0005..\n",
      "Epoch: 745/1500..  Training Loss: -7.381..  Test Loss: -7.384..  Learning rate: 0.0005..\n",
      "Epoch: 746/1500..  Training Loss: -7.294..  Test Loss: -7.346..  Learning rate: 0.0005..\n",
      "Epoch: 747/1500..  Training Loss: -7.299..  Test Loss: -7.288..  Learning rate: 0.0005..\n",
      "Epoch: 748/1500..  Training Loss: -7.367..  Test Loss: -7.422..  Learning rate: 0.0005..\n",
      "Epoch: 749/1500..  Training Loss: -7.359..  Test Loss: -7.364..  Learning rate: 0.0005..\n",
      "Epoch: 750/1500..  Training Loss: -7.358..  Test Loss: -7.397..  Learning rate: 0.0005..\n",
      "Epoch: 751/1500..  Training Loss: -7.337..  Test Loss: -7.198..  Learning rate: 0.0005..\n",
      "Epoch: 752/1500..  Training Loss: -7.309..  Test Loss: -7.363..  Learning rate: 0.0005..\n",
      "Epoch: 753/1500..  Training Loss: -7.305..  Test Loss: -7.410..  Learning rate: 0.0005..\n",
      "Epoch: 754/1500..  Training Loss: -7.322..  Test Loss: -7.436..  Learning rate: 0.0005..\n",
      "Epoch: 755/1500..  Training Loss: -7.349..  Test Loss: -7.377..  Learning rate: 0.0005..\n",
      "Epoch: 756/1500..  Training Loss: -7.375..  Test Loss: -7.332..  Learning rate: 0.0005..\n",
      "Epoch: 757/1500..  Training Loss: -7.360..  Test Loss: -7.041..  Learning rate: 0.0005..\n",
      "Epoch: 758/1500..  Training Loss: -7.330..  Test Loss: -7.427..  Learning rate: 0.0005..\n",
      "Epoch: 759/1500..  Training Loss: -7.341..  Test Loss: -7.284..  Learning rate: 0.0005..\n",
      "Epoch: 760/1500..  Training Loss: -7.324..  Test Loss: -7.401..  Learning rate: 0.0005..\n",
      "Epoch: 761/1500..  Training Loss: -7.365..  Test Loss: -7.396..  Learning rate: 0.0005..\n",
      "Epoch: 762/1500..  Training Loss: -7.387..  Test Loss: -7.076..  Learning rate: 0.0005..\n",
      "Epoch: 763/1500..  Training Loss: -7.328..  Test Loss: -7.221..  Learning rate: 0.0005..\n",
      "Epoch: 764/1500..  Training Loss: -7.353..  Test Loss: -7.404..  Learning rate: 0.0005..\n",
      "Epoch: 765/1500..  Training Loss: -7.357..  Test Loss: -7.433..  Learning rate: 0.0005..\n",
      "Epoch: 766/1500..  Training Loss: -7.330..  Test Loss: -7.325..  Learning rate: 0.0005..\n",
      "Epoch: 767/1500..  Training Loss: -7.365..  Test Loss: -7.250..  Learning rate: 0.0005..\n",
      "Epoch: 768/1500..  Training Loss: -7.374..  Test Loss: -7.423..  Learning rate: 0.0005..\n",
      "Epoch: 769/1500..  Training Loss: -7.319..  Test Loss: -7.230..  Learning rate: 0.0005..\n",
      "Epoch: 770/1500..  Training Loss: -7.355..  Test Loss: -7.373..  Learning rate: 0.0005..\n",
      "Epoch: 771/1500..  Training Loss: -7.376..  Test Loss: -7.026..  Learning rate: 0.0005..\n",
      "Epoch: 772/1500..  Training Loss: -7.372..  Test Loss: -7.102..  Learning rate: 0.0005..\n",
      "Epoch: 773/1500..  Training Loss: -7.350..  Test Loss: -7.030..  Learning rate: 0.0005..\n",
      "Epoch: 774/1500..  Training Loss: -7.272..  Test Loss: -7.417..  Learning rate: 0.0005..\n",
      "Epoch: 775/1500..  Training Loss: -7.386..  Test Loss: -7.344..  Learning rate: 0.0005..\n",
      "Epoch: 776/1500..  Training Loss: -7.315..  Test Loss: -7.349..  Learning rate: 0.0005..\n",
      "Epoch: 777/1500..  Training Loss: -7.321..  Test Loss: -7.061..  Learning rate: 0.0005..\n",
      "Epoch: 778/1500..  Training Loss: -7.287..  Test Loss: -7.389..  Learning rate: 0.0005..\n",
      "Epoch: 779/1500..  Training Loss: -7.341..  Test Loss: -7.418..  Learning rate: 0.0005..\n",
      "Epoch: 780/1500..  Training Loss: -7.324..  Test Loss: -7.343..  Learning rate: 0.0005..\n",
      "Epoch: 781/1500..  Training Loss: -7.300..  Test Loss: -7.340..  Learning rate: 0.0005..\n",
      "Epoch: 782/1500..  Training Loss: -7.390..  Test Loss: -7.375..  Learning rate: 0.0005..\n",
      "Epoch: 783/1500..  Training Loss: -7.309..  Test Loss: -7.278..  Learning rate: 0.0005..\n",
      "Epoch: 784/1500..  Training Loss: -7.290..  Test Loss: -7.408..  Learning rate: 0.0005..\n",
      "Epoch: 785/1500..  Training Loss: -7.363..  Test Loss: -7.377..  Learning rate: 0.0005..\n",
      "Epoch: 786/1500..  Training Loss: -7.374..  Test Loss: -7.414..  Learning rate: 0.0005..\n",
      "Epoch: 787/1500..  Training Loss: -7.370..  Test Loss: -7.388..  Learning rate: 0.0005..\n",
      "Epoch: 788/1500..  Training Loss: -7.297..  Test Loss: -7.205..  Learning rate: 0.0005..\n",
      "Epoch: 789/1500..  Training Loss: -7.368..  Test Loss: -7.407..  Learning rate: 0.0005..\n",
      "Epoch: 790/1500..  Training Loss: -7.293..  Test Loss: -7.310..  Learning rate: 0.0005..\n",
      "Epoch: 791/1500..  Training Loss: -7.381..  Test Loss: -7.278..  Learning rate: 0.0005..\n",
      "Epoch: 792/1500..  Training Loss: -7.283..  Test Loss: -7.360..  Learning rate: 0.0005..\n",
      "Epoch: 793/1500..  Training Loss: -7.332..  Test Loss: -7.268..  Learning rate: 0.0005..\n",
      "Epoch: 794/1500..  Training Loss: -7.357..  Test Loss: -7.440..  Learning rate: 0.0005..\n",
      "Epoch: 795/1500..  Training Loss: -7.216..  Test Loss: -7.352..  Learning rate: 0.0005..\n",
      "Epoch: 796/1500..  Training Loss: -7.309..  Test Loss: -7.019..  Learning rate: 0.0005..\n",
      "Epoch: 797/1500..  Training Loss: -7.319..  Test Loss: -7.373..  Learning rate: 0.0005..\n",
      "Epoch: 798/1500..  Training Loss: -7.313..  Test Loss: -7.372..  Learning rate: 0.0005..\n",
      "Epoch: 799/1500..  Training Loss: -7.255..  Test Loss: -7.336..  Learning rate: 0.0005..\n",
      "Epoch: 800/1500..  Training Loss: -7.350..  Test Loss: -6.849..  Learning rate: 0.0005..\n",
      "Epoch: 801/1500..  Training Loss: -7.353..  Test Loss: -7.406..  Learning rate: 0.0005..\n",
      "Epoch: 802/1500..  Training Loss: -7.318..  Test Loss: -7.406..  Learning rate: 0.0005..\n",
      "Epoch: 803/1500..  Training Loss: -7.401..  Test Loss: -6.952..  Learning rate: 0.0005..\n",
      "Epoch: 804/1500..  Training Loss: -7.313..  Test Loss: -7.400..  Learning rate: 0.0005..\n",
      "Epoch: 805/1500..  Training Loss: -7.382..  Test Loss: -7.360..  Learning rate: 0.0005..\n",
      "Epoch: 806/1500..  Training Loss: -7.310..  Test Loss: -7.390..  Learning rate: 0.0005..\n",
      "Epoch: 807/1500..  Training Loss: -7.332..  Test Loss: -7.414..  Learning rate: 0.0005..\n",
      "Epoch: 808/1500..  Training Loss: -7.366..  Test Loss: -7.374..  Learning rate: 0.0005..\n",
      "Epoch: 809/1500..  Training Loss: -7.329..  Test Loss: -7.410..  Learning rate: 0.0005..\n",
      "Epoch: 810/1500..  Training Loss: -7.353..  Test Loss: -7.415..  Learning rate: 0.0005..\n",
      "Epoch: 811/1500..  Training Loss: -7.370..  Test Loss: -7.373..  Learning rate: 0.0005..\n",
      "Epoch: 812/1500..  Training Loss: -7.311..  Test Loss: -7.260..  Learning rate: 0.0005..\n",
      "Epoch: 813/1500..  Training Loss: -7.390..  Test Loss: -7.311..  Learning rate: 0.0005..\n",
      "Epoch: 814/1500..  Training Loss: -7.375..  Test Loss: -7.423..  Learning rate: 0.0005..\n",
      "Epoch: 815/1500..  Training Loss: -7.321..  Test Loss: -7.417..  Learning rate: 0.0005..\n",
      "Epoch: 816/1500..  Training Loss: -7.303..  Test Loss: -7.316..  Learning rate: 0.0005..\n",
      "Epoch: 817/1500..  Training Loss: -7.359..  Test Loss: -7.441..  Learning rate: 0.0005..\n",
      "Epoch: 818/1500..  Training Loss: -7.346..  Test Loss: -7.425..  Learning rate: 0.0005..\n",
      "Epoch: 819/1500..  Training Loss: -7.358..  Test Loss: -7.389..  Learning rate: 0.0005..\n",
      "Epoch: 820/1500..  Training Loss: -7.360..  Test Loss: -7.361..  Learning rate: 0.0005..\n",
      "Epoch: 821/1500..  Training Loss: -7.360..  Test Loss: -7.400..  Learning rate: 0.0005..\n",
      "Epoch: 822/1500..  Training Loss: -7.331..  Test Loss: -7.355..  Learning rate: 0.0005..\n",
      "Epoch: 823/1500..  Training Loss: -7.343..  Test Loss: -7.267..  Learning rate: 0.0005..\n",
      "Epoch: 824/1500..  Training Loss: -7.282..  Test Loss: -7.389..  Learning rate: 0.0005..\n",
      "Epoch: 825/1500..  Training Loss: -7.309..  Test Loss: -7.230..  Learning rate: 0.0005..\n",
      "Epoch: 826/1500..  Training Loss: -7.369..  Test Loss: -7.331..  Learning rate: 0.0005..\n",
      "Epoch: 827/1500..  Training Loss: -7.388..  Test Loss: -7.214..  Learning rate: 0.0005..\n",
      "Epoch: 828/1500..  Training Loss: -7.301..  Test Loss: -7.123..  Learning rate: 0.0005..\n",
      "Epoch: 829/1500..  Training Loss: -7.336..  Test Loss: -7.331..  Learning rate: 0.0005..\n",
      "Epoch: 830/1500..  Training Loss: -7.351..  Test Loss: -7.342..  Learning rate: 0.0005..\n",
      "Epoch: 831/1500..  Training Loss: -7.366..  Test Loss: -7.369..  Learning rate: 0.0005..\n",
      "Epoch: 832/1500..  Training Loss: -7.298..  Test Loss: -7.162..  Learning rate: 0.0005..\n",
      "Epoch: 833/1500..  Training Loss: -7.336..  Test Loss: -7.271..  Learning rate: 0.0005..\n",
      "Epoch: 834/1500..  Training Loss: -7.342..  Test Loss: -7.314..  Learning rate: 0.0005..\n",
      "Epoch: 835/1500..  Training Loss: -7.373..  Test Loss: -7.298..  Learning rate: 0.0005..\n",
      "Epoch: 836/1500..  Training Loss: -7.353..  Test Loss: -6.799..  Learning rate: 0.0005..\n",
      "Epoch: 837/1500..  Training Loss: -7.291..  Test Loss: -7.372..  Learning rate: 0.0005..\n",
      "Epoch: 838/1500..  Training Loss: -7.360..  Test Loss: -7.434..  Learning rate: 0.0005..\n",
      "Epoch: 839/1500..  Training Loss: -7.395..  Test Loss: -7.452..  Learning rate: 0.0005..\n",
      "Epoch: 840/1500..  Training Loss: -7.364..  Test Loss: -7.345..  Learning rate: 0.0005..\n",
      "Epoch: 841/1500..  Training Loss: -7.383..  Test Loss: -7.320..  Learning rate: 0.0005..\n",
      "Epoch: 842/1500..  Training Loss: -7.372..  Test Loss: -7.415..  Learning rate: 0.0005..\n",
      "Epoch: 843/1500..  Training Loss: -7.325..  Test Loss: -7.380..  Learning rate: 0.0005..\n",
      "Epoch: 844/1500..  Training Loss: -7.382..  Test Loss: -7.363..  Learning rate: 0.0005..\n",
      "Epoch: 845/1500..  Training Loss: -7.374..  Test Loss: -6.985..  Learning rate: 0.0005..\n",
      "Epoch: 846/1500..  Training Loss: -7.319..  Test Loss: -7.360..  Learning rate: 0.0005..\n",
      "Epoch: 847/1500..  Training Loss: -7.198..  Test Loss: -7.170..  Learning rate: 0.0005..\n",
      "Epoch: 848/1500..  Training Loss: -7.321..  Test Loss: -7.444..  Learning rate: 0.0005..\n",
      "Epoch: 849/1500..  Training Loss: -7.366..  Test Loss: -7.386..  Learning rate: 0.0005..\n",
      "Epoch: 850/1500..  Training Loss: -7.328..  Test Loss: -7.380..  Learning rate: 0.0005..\n",
      "Epoch: 851/1500..  Training Loss: -7.345..  Test Loss: -7.315..  Learning rate: 0.0005..\n",
      "Epoch: 852/1500..  Training Loss: -7.337..  Test Loss: -7.374..  Learning rate: 0.0005..\n",
      "Epoch: 853/1500..  Training Loss: -7.378..  Test Loss: -7.431..  Learning rate: 0.0005..\n",
      "Epoch: 854/1500..  Training Loss: -7.358..  Test Loss: -7.276..  Learning rate: 0.0005..\n",
      "Epoch: 855/1500..  Training Loss: -7.353..  Test Loss: -7.333..  Learning rate: 0.0005..\n",
      "Epoch: 856/1500..  Training Loss: -7.357..  Test Loss: -7.206..  Learning rate: 0.0005..\n",
      "Epoch: 857/1500..  Training Loss: -7.362..  Test Loss: -7.397..  Learning rate: 0.0005..\n",
      "Epoch: 858/1500..  Training Loss: -7.397..  Test Loss: -7.075..  Learning rate: 0.0005..\n",
      "Epoch: 859/1500..  Training Loss: -7.348..  Test Loss: -7.389..  Learning rate: 0.0005..\n",
      "Epoch: 860/1500..  Training Loss: -7.312..  Test Loss: -7.395..  Learning rate: 0.0005..\n",
      "Epoch: 861/1500..  Training Loss: -7.399..  Test Loss: -7.444..  Learning rate: 0.0005..\n",
      "Epoch: 862/1500..  Training Loss: -7.279..  Test Loss: -7.372..  Learning rate: 0.0005..\n",
      "Epoch: 863/1500..  Training Loss: -7.391..  Test Loss: -7.436..  Learning rate: 0.0005..\n",
      "Epoch: 864/1500..  Training Loss: -7.332..  Test Loss: -7.337..  Learning rate: 0.0005..\n",
      "Epoch: 865/1500..  Training Loss: -7.366..  Test Loss: -6.929..  Learning rate: 0.0005..\n",
      "Epoch: 866/1500..  Training Loss: -7.363..  Test Loss: -7.229..  Learning rate: 0.0005..\n",
      "Epoch: 867/1500..  Training Loss: -7.324..  Test Loss: -7.304..  Learning rate: 0.0005..\n",
      "Epoch: 868/1500..  Training Loss: -7.317..  Test Loss: -7.306..  Learning rate: 0.0005..\n",
      "Epoch: 869/1500..  Training Loss: -7.389..  Test Loss: -7.347..  Learning rate: 0.0005..\n",
      "Epoch: 870/1500..  Training Loss: -7.352..  Test Loss: -7.349..  Learning rate: 0.0005..\n",
      "Epoch: 871/1500..  Training Loss: -7.333..  Test Loss: -7.272..  Learning rate: 0.0005..\n",
      "Epoch: 872/1500..  Training Loss: -7.341..  Test Loss: -7.389..  Learning rate: 0.0005..\n",
      "Epoch: 873/1500..  Training Loss: -7.347..  Test Loss: -7.435..  Learning rate: 0.0005..\n",
      "Epoch: 874/1500..  Training Loss: -7.320..  Test Loss: -7.461..  Learning rate: 0.0005..\n",
      "Epoch: 875/1500..  Training Loss: -7.329..  Test Loss: -7.408..  Learning rate: 0.0005..\n",
      "Epoch: 876/1500..  Training Loss: -7.363..  Test Loss: -7.355..  Learning rate: 0.0005..\n",
      "Epoch: 877/1500..  Training Loss: -7.343..  Test Loss: -7.395..  Learning rate: 0.0005..\n",
      "Epoch: 878/1500..  Training Loss: -7.374..  Test Loss: -7.419..  Learning rate: 0.0005..\n",
      "Epoch: 879/1500..  Training Loss: -7.337..  Test Loss: -7.404..  Learning rate: 0.0005..\n",
      "Epoch: 880/1500..  Training Loss: -7.288..  Test Loss: -7.042..  Learning rate: 0.0005..\n",
      "Epoch: 881/1500..  Training Loss: -7.349..  Test Loss: -7.353..  Learning rate: 0.0005..\n",
      "Epoch: 882/1500..  Training Loss: -7.307..  Test Loss: -7.214..  Learning rate: 0.0005..\n",
      "Epoch: 883/1500..  Training Loss: -7.344..  Test Loss: -7.378..  Learning rate: 0.0005..\n",
      "Epoch: 884/1500..  Training Loss: -7.331..  Test Loss: -7.413..  Learning rate: 0.0005..\n",
      "Epoch: 885/1500..  Training Loss: -7.341..  Test Loss: -7.428..  Learning rate: 0.0005..\n",
      "Epoch: 886/1500..  Training Loss: -7.313..  Test Loss: -7.358..  Learning rate: 0.0005..\n",
      "Epoch: 887/1500..  Training Loss: -7.329..  Test Loss: -7.406..  Learning rate: 0.0005..\n",
      "Epoch: 888/1500..  Training Loss: -7.344..  Test Loss: -7.221..  Learning rate: 0.0005..\n",
      "Epoch: 889/1500..  Training Loss: -7.393..  Test Loss: -7.338..  Learning rate: 0.0005..\n",
      "Epoch: 890/1500..  Training Loss: -7.289..  Test Loss: -7.163..  Learning rate: 0.0005..\n",
      "Epoch: 891/1500..  Training Loss: -7.341..  Test Loss: -7.432..  Learning rate: 0.0005..\n",
      "Epoch: 892/1500..  Training Loss: -7.292..  Test Loss: -7.051..  Learning rate: 0.0005..\n",
      "Epoch: 893/1500..  Training Loss: -7.359..  Test Loss: -7.342..  Learning rate: 0.0005..\n",
      "Epoch: 894/1500..  Training Loss: -7.357..  Test Loss: -7.083..  Learning rate: 0.0005..\n",
      "Epoch: 895/1500..  Training Loss: -7.318..  Test Loss: -7.399..  Learning rate: 0.0005..\n",
      "Epoch: 896/1500..  Training Loss: -7.376..  Test Loss: -7.321..  Learning rate: 0.0005..\n",
      "Epoch: 897/1500..  Training Loss: -7.343..  Test Loss: -7.341..  Learning rate: 0.0005..\n",
      "Epoch: 898/1500..  Training Loss: -7.361..  Test Loss: -7.280..  Learning rate: 0.0005..\n",
      "Epoch: 899/1500..  Training Loss: -7.365..  Test Loss: -7.419..  Learning rate: 0.0005..\n",
      "Epoch: 900/1500..  Training Loss: -7.337..  Test Loss: -7.138..  Learning rate: 0.0005..\n",
      "Epoch: 901/1500..  Training Loss: -7.331..  Test Loss: -7.394..  Learning rate: 0.0005..\n",
      "Epoch: 902/1500..  Training Loss: -7.322..  Test Loss: -7.399..  Learning rate: 0.0005..\n",
      "Epoch: 903/1500..  Training Loss: -7.334..  Test Loss: -7.463..  Learning rate: 0.0005..\n",
      "Epoch: 904/1500..  Training Loss: -7.348..  Test Loss: -7.416..  Learning rate: 0.0005..\n",
      "Epoch: 905/1500..  Training Loss: -7.380..  Test Loss: -7.287..  Learning rate: 0.0005..\n",
      "Epoch: 906/1500..  Training Loss: -7.390..  Test Loss: -7.419..  Learning rate: 0.0005..\n",
      "Epoch: 907/1500..  Training Loss: -7.282..  Test Loss: -7.075..  Learning rate: 0.0005..\n",
      "Epoch: 908/1500..  Training Loss: -7.341..  Test Loss: -7.421..  Learning rate: 0.0005..\n",
      "Epoch: 909/1500..  Training Loss: -7.394..  Test Loss: -7.225..  Learning rate: 0.0005..\n",
      "Epoch: 910/1500..  Training Loss: -7.311..  Test Loss: -7.115..  Learning rate: 0.0005..\n",
      "Epoch: 911/1500..  Training Loss: -7.370..  Test Loss: -7.416..  Learning rate: 0.0005..\n",
      "Epoch: 912/1500..  Training Loss: -7.348..  Test Loss: -7.141..  Learning rate: 0.0005..\n",
      "Epoch: 913/1500..  Training Loss: -7.313..  Test Loss: -7.418..  Learning rate: 0.0005..\n",
      "Epoch: 914/1500..  Training Loss: -7.313..  Test Loss: -7.345..  Learning rate: 0.0005..\n",
      "Epoch: 915/1500..  Training Loss: -7.351..  Test Loss: -7.374..  Learning rate: 0.0005..\n",
      "Epoch: 916/1500..  Training Loss: -7.372..  Test Loss: -7.452..  Learning rate: 0.0005..\n",
      "Epoch: 917/1500..  Training Loss: -7.401..  Test Loss: -7.262..  Learning rate: 0.0005..\n",
      "Epoch: 918/1500..  Training Loss: -7.318..  Test Loss: -6.805..  Learning rate: 0.0005..\n",
      "Epoch: 919/1500..  Training Loss: -7.300..  Test Loss: -7.412..  Learning rate: 0.0005..\n",
      "Epoch: 920/1500..  Training Loss: -7.273..  Test Loss: -7.346..  Learning rate: 0.0005..\n",
      "Epoch: 921/1500..  Training Loss: -7.356..  Test Loss: -7.043..  Learning rate: 0.0005..\n",
      "Epoch: 922/1500..  Training Loss: -7.307..  Test Loss: -7.459..  Learning rate: 0.0005..\n",
      "Epoch: 923/1500..  Training Loss: -7.327..  Test Loss: -6.751..  Learning rate: 0.0005..\n",
      "Epoch: 924/1500..  Training Loss: -7.334..  Test Loss: -7.383..  Learning rate: 0.0005..\n",
      "Epoch: 925/1500..  Training Loss: -7.359..  Test Loss: -7.240..  Learning rate: 0.0005..\n",
      "Epoch: 926/1500..  Training Loss: -7.380..  Test Loss: -7.446..  Learning rate: 0.0005..\n",
      "Epoch: 927/1500..  Training Loss: -7.373..  Test Loss: -7.353..  Learning rate: 0.0005..\n",
      "Epoch: 928/1500..  Training Loss: -7.313..  Test Loss: -7.379..  Learning rate: 0.0005..\n",
      "Epoch: 929/1500..  Training Loss: -7.354..  Test Loss: -7.433..  Learning rate: 0.0005..\n",
      "Epoch: 930/1500..  Training Loss: -7.272..  Test Loss: -7.416..  Learning rate: 0.0005..\n",
      "Epoch: 931/1500..  Training Loss: -7.303..  Test Loss: -7.380..  Learning rate: 0.0005..\n",
      "Epoch: 932/1500..  Training Loss: -7.371..  Test Loss: -7.424..  Learning rate: 0.0005..\n",
      "Epoch: 933/1500..  Training Loss: -7.321..  Test Loss: -7.244..  Learning rate: 0.0005..\n",
      "Epoch: 934/1500..  Training Loss: -7.399..  Test Loss: -7.451..  Learning rate: 0.0005..\n",
      "Epoch: 935/1500..  Training Loss: -7.362..  Test Loss: -7.409..  Learning rate: 0.0005..\n",
      "Epoch: 936/1500..  Training Loss: -7.282..  Test Loss: -7.222..  Learning rate: 0.0005..\n",
      "Epoch: 937/1500..  Training Loss: -7.356..  Test Loss: -7.392..  Learning rate: 0.0005..\n",
      "Epoch: 938/1500..  Training Loss: -7.369..  Test Loss: -7.207..  Learning rate: 0.0005..\n",
      "Epoch: 939/1500..  Training Loss: -7.383..  Test Loss: -7.337..  Learning rate: 0.0005..\n",
      "Epoch: 940/1500..  Training Loss: -7.373..  Test Loss: -7.357..  Learning rate: 0.0005..\n",
      "Epoch: 941/1500..  Training Loss: -7.327..  Test Loss: -7.363..  Learning rate: 0.0005..\n",
      "Epoch: 942/1500..  Training Loss: -7.355..  Test Loss: -7.451..  Learning rate: 0.0005..\n",
      "Epoch: 943/1500..  Training Loss: -7.394..  Test Loss: -7.385..  Learning rate: 0.0005..\n",
      "Epoch: 944/1500..  Training Loss: -7.379..  Test Loss: -7.461..  Learning rate: 0.0005..\n",
      "Epoch: 945/1500..  Training Loss: -7.306..  Test Loss: -7.335..  Learning rate: 0.0005..\n",
      "Epoch: 946/1500..  Training Loss: -7.404..  Test Loss: -7.449..  Learning rate: 0.0005..\n",
      "Epoch: 947/1500..  Training Loss: -7.352..  Test Loss: -7.344..  Learning rate: 0.0005..\n",
      "Epoch: 948/1500..  Training Loss: -7.347..  Test Loss: -7.433..  Learning rate: 0.0005..\n",
      "Epoch: 949/1500..  Training Loss: -7.328..  Test Loss: -7.273..  Learning rate: 0.0005..\n",
      "Epoch: 950/1500..  Training Loss: -7.435..  Test Loss: -7.362..  Learning rate: 0.0005..\n",
      "Epoch: 951/1500..  Training Loss: -7.379..  Test Loss: -7.399..  Learning rate: 0.0005..\n",
      "Epoch: 952/1500..  Training Loss: -7.379..  Test Loss: -7.391..  Learning rate: 0.0005..\n",
      "Epoch: 953/1500..  Training Loss: -7.315..  Test Loss: -7.388..  Learning rate: 0.0005..\n",
      "Epoch: 954/1500..  Training Loss: -7.377..  Test Loss: -7.077..  Learning rate: 0.0005..\n"
     ]
    }
   ],
   "source": [
    "epochs = 1500\n",
    "test_loss_min = np.Inf\n",
    "steps = 0\n",
    "model.apply(init_weights)\n",
    "model.to('cuda')\n",
    "train_losses, test_losses, learningRate = [], [], []\n",
    "#vg = ValidationGradient(test_loader, criterion, 'cuda')\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=500, gamma=0.5)\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    for features, labels in train_loader:\n",
    "        optimizer.zero_grad() # dé-commenter si on utilise pas Adatune\n",
    "        features,labels = features.to('cuda'), labels.to('cuda')\n",
    "        MSE = model(features)\n",
    "        loss = criterion(MSE, labels)\n",
    "\n",
    "        # a commenter si on utilise pas adatune\n",
    "        ###\n",
    "        \"\"\"first_grad = ag.grad(loss, model.parameters(), create_graph=True, retain_graph=True)\n",
    "        hyper_optim.compute_hg(model, first_grad)\n",
    "        for params, gradients in zip(model.parameters(), first_grad):\n",
    "            params.grad = gradients\n",
    "        optimizer.step()\n",
    "        hyper_optim.hyper_step(vg.val_grad(model))\n",
    "        clear_grad(model)\"\"\"\n",
    "        ###\n",
    "        \n",
    "        # décommenter pour utiliser adamHD\n",
    "        loss.backward()\n",
    "        #torch.nn.utils.clip_grad_norm_(model.parameters(), 2000)\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    else:\n",
    "        test_loss = 0\n",
    "\n",
    "        \n",
    "        # Turn off gradients for validation, saves memory and computations\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            for features, labels in test_loader:\n",
    "                features,labels = features.to('cuda'), labels.to('cuda')\n",
    "                MSE = model(features)\n",
    "                test_loss += criterion(MSE, labels)\n",
    "\n",
    "        model.train()        \n",
    "        train_losses.append(running_loss/len(train_loader))\n",
    "        test_loss = test_loss/len(test_loader)\n",
    "        test_losses.append(test_loss/len(test_loader))\n",
    "        learningRate.append(optimizer.param_groups[0]['lr'])\n",
    "        #scheduler.step(test_loss)\n",
    "        scheduler.step()\n",
    "        print(\"Epoch: {}/{}.. \".format(e+1, epochs),\n",
    "              \"Training Loss: {:.3f}.. \".format(np.log(running_loss/len(train_loader))),\n",
    "              \"Test Loss: {:.3f}.. \".format(torch.log(test_loss)),\n",
    "              \"Learning rate: {}..\".format(optimizer.param_groups[0]['lr']))\n",
    "        # save model if validation loss has decreased\n",
    "        if test_loss <= test_loss_min:\n",
    "            print('Validation loss decreased ({} --> {}).  Saving model ...'.format(test_loss_min,test_loss))\n",
    "            torch.save(model.state_dict(), 'model5Percent.pt')\n",
    "            test_loss_min = test_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LSE_MdW3q1Ve"
   },
   "source": [
    "The result obtained are a little better than the one reached by the paper only using uniform random sampling. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yn5zZuunrDEV"
   },
   "source": [
    "![best_result](images/resultat-implied-volatility-BS.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is save under the name `modelvolatilityBS.pt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-uK3La0vs-lo"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4E5wrJZEtBek"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "DNN-implicite-volatility.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
